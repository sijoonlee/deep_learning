{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SPAM_Classification_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uW8XHZIlHxG2",
        "colab": {}
      },
      "source": [
        "#!tar -xopf eron.tar\n",
        "!tar -zxvf data.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9J3UlzLCU9Hj",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "class FileReader(object):\n",
        "  def __init__(self):\n",
        "    self.ham = []\n",
        "    self.spam = []\n",
        "    self.ham_paths = [\"enron1/ham/*.txt\", \"enron2/ham/*.txt\", \"enron3/ham/*.txt\", \"enron4/ham/*.txt\", \"enron5/ham/*.txt\", \"enron6/ham/*.txt\"]\n",
        "    self.spam_paths = [\"enron1/spam/*.txt\", \"enron2/spam/*.txt\", \"enron3/spam/*.txt\", \"enron4/spam/*.txt\", \"enron5/spam/*.txt\", \"enron6/spam/*.txt\"]\n",
        "  \n",
        "  def read_file(self, path, minimum_word_count = 3, unnecessary =  [\"-\", \".\", \",\", \"/\", \":\", \"@\", \"'\", \"!\"]):\n",
        "    files  = glob.glob(path)\n",
        "    content_list = []\n",
        "    for file in files:\n",
        "        with open(file, encoding=\"ISO-8859-1\") as f:\n",
        "            content = f.read()\n",
        "            if len(content.split()) > minimum_word_count:      \n",
        "              content = content.lower()\n",
        "              if len(unnecessary) is not 0:\n",
        "                  content = ''.join([c for c in content if c not in unnecessary])\n",
        "              content_list.append(content)\n",
        "    return content_list\n",
        "  \n",
        "  def truncate_before_combine(self, data, maximum_length = 5000):\n",
        "    if maximum_length is not 0:\n",
        "      if len(data) > maximum_length:\n",
        "        random.shuffle(data)\n",
        "        data = data[:maximum_length]\n",
        "    return data\n",
        "  \n",
        "  def load_ham_and_spam(self, ham_paths = \"default\", spam_paths = \"default\", truncation_length = 5000): # 0 for no truncation\n",
        "    \n",
        "    if ham_paths == \"default\":\n",
        "      ham_paths = self.ham_paths\n",
        "    if spam_paths == \"default\":\n",
        "      spam_paths = self.spam_paths\n",
        "    \n",
        "    self.ham = [ item for path in ham_paths for item in self.read_file(path) ]\n",
        "    if truncation_length != 0:\n",
        "      self.ham = self.truncate_before_combine(self.ham, truncation_length)\n",
        "    print(\"ham length \", len(self.ham))\n",
        "    \n",
        "    self.spam = [item for path in spam_paths for item in self.read_file(path) ]\n",
        "    if truncation_length != 0:\n",
        "      self.spam = self.truncate_before_combine(self.spam, truncation_length)\n",
        "    print(\"spam length \", len(self.spam))\n",
        "    \n",
        "    data = self.ham + self.spam\n",
        "    \n",
        "    ham_label = [0 for _ in range(len(self.ham))]\n",
        "    spam_label = [1 for _ in range(len(self.spam))]\n",
        "    \n",
        "    label_tensor = torch.as_tensor(ham_label + spam_label, dtype = torch.int16)\n",
        "    \n",
        "    return data, label_tensor\n",
        "  \n",
        "  def print_sample(self, which =\"both\"): # ham, spam or both\n",
        "    if which == \"ham\" or which == \"both\":\n",
        "      idx = random.randint(0, len(self.ham))\n",
        "      print(\"----------- ham sample -------------\")\n",
        "      print(self.ham[idx])\n",
        "    if which == \"spam\" or which == \"both\":\n",
        "      idx = random.randint(0, len(self.spam))\n",
        "      print(\"----------- spam sample -------------\")\n",
        "      print(self.spam[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5j3m0dUddcgP",
        "outputId": "eff14cc5-864d-4e09-d95e-8a933f695e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "reader = FileReader()\n",
        "\n",
        "data, label = reader.load_ham_and_spam(\"default\", \"default\", truncation_length = 0)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ham length  16540\n",
            "spam length  17108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wg5nJxOMxVQW",
        "outputId": "b50b3b2a-ce3e-4f9b-984d-850f715ba581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "reader.print_sample()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------- ham sample -------------\n",
            "subject on  call notes\n",
            "please find attached the on  call notes for the weekend of 3  10 & 11  01 \n",
            "bob\n",
            "----------- spam sample -------------\n",
            "subject greater than some not\n",
            "r e finance before election when the r a tes will rise \n",
            "it is your last chance\n",
            "http    www  aonmate  com \n",
            "you are already approv e d with 3  0 point\n",
            "thank you \n",
            "beard\n",
            "          \n",
            "we fastidious  at of emerald\n",
            "compost an or the nigger quonset\n",
            "cantle duchess is it doodle\n",
            "counselor headlight  in gripe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d3JqHhaQcWmY",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "class Vocab_to_int(object):\n",
        "\n",
        "  def __init__(self, saved_dir='./', file_name=\"vocab_to_int.csv\"):\n",
        "      os.makedirs(saved_dir, exist_ok=True)\n",
        "      self.path = os.path.join(saved_dir, file_name)\n",
        "\n",
        "  def save_file(self, vocab_to_int):\n",
        "      df = pd.DataFrame(list(vocab_to_int.items()))\n",
        "      df.dropna(inplace=True)\n",
        "      df = df.T\n",
        "      df.to_csv(self.path, index=False, header=False)\n",
        "      print(\"saved as\", self.path)\n",
        "\n",
        "  def open_file(self, path = \"Default\"):\n",
        "      if path == \"Default\":\n",
        "        path = self.path\n",
        "      df = pd.read_csv(path)\n",
        "      df.dropna(inplace=True)\n",
        "      dict = df.to_dict('records')[0]\n",
        "      return dict\n",
        "\n",
        "  def generate(self, seqs, save_file=True):\n",
        "      vocabs = [vocab for seq in seqs for vocab in seq.split()]\n",
        "      # a = [  word for seq in [\"a d\",\"b d\",\"c d\"] for word in seq.split() ]\n",
        "      # ['a', 'd', 'b', 'd', 'c', 'd']\n",
        "\n",
        "      # Count word frequency\n",
        "      # Counter({'the': 39770, 'to': 32356, 'and': 22835, .....\n",
        "      vocab_count = Counter(vocabs)\n",
        "\n",
        "      vocab_count = vocab_count.most_common(len(vocab_count))\n",
        "\n",
        "      vocab_to_int = {word : index+2 for index, (word, count) in enumerate(vocab_count)}\n",
        "      vocab_to_int.update({'__PADDING__': 0}) # index 0 for padding\n",
        "      vocab_to_int.update({'__UNKNOWN__': 1}) # index 1 for unknown word such as broken character\n",
        "\n",
        "      if save_file:\n",
        "        self.save_file(vocab_to_int)\n",
        "\n",
        "      return vocab_to_int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IcyiNGm3cZHS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ca0c352-eb4a-4678-efa8-d92104068b0b"
      },
      "source": [
        "vti = Vocab_to_int()\n",
        "vocab_to_int = vti.generate(data, save_file=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saved as ./vocab_to_int.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Vdi2zuKHYue",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class Vectorizer(object):\n",
        "  ''' \n",
        "  Using vocab_to_int dict, \n",
        "  Change words into integers in string data\n",
        "  '''\n",
        "  \n",
        "  def __init__(self, vocab_to_int):\n",
        "    self.vocab_to_int = vocab_to_int\n",
        "    \n",
        "  def vectorize_seqs(self, seqs):\n",
        "    # Vectorize each sequence\n",
        "    vectorized_seqs = []\n",
        "    for seq in seqs: \n",
        "      vectorized_seqs.append([self.vocab_to_int.get(word, 1) for word in seq.split()])\n",
        "      # self.vocab_to_int.get(word, 1) ; mean if no value for key, it will return 1 (unknown)\n",
        "    return vectorized_seqs\n",
        "  \n",
        "  def add_padding(self, vectorized_seqs, seq_lengths):\n",
        "    '''\n",
        "    The length of the seq_tensor is the length of the longest sentence in the data\n",
        "    The shorter sentences will have padding(zero) at the their ends\n",
        "    '''\n",
        "    seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
        "    for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "      seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "    return seq_tensor\n",
        "  \n",
        "  def vectorize(self, seqs):\n",
        "    vectorized_seqs = self.vectorize_seqs(seqs)\n",
        "    seq_lengths = torch.LongTensor(list(map(len, vectorized_seqs)))\n",
        "    seq_tensor = self.add_padding(vectorized_seqs, seq_lengths)\n",
        "    \n",
        "    return seq_tensor, seq_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pA1eZE5mouSo",
        "colab": {}
      },
      "source": [
        "v = Vectorizer(vocab_to_int)\n",
        "seq_tensor, seq_lengths = v.vectorize(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DInEdI1M3IOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PT6HTGm3HYu7",
        "colab": {}
      },
      "source": [
        "class DataDivider(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        init with seq_tensor and label, both of them are torch.tensor\n",
        "        '''        \n",
        "    def check_length(self, seq_tensor, seq_lengths, label):\n",
        "        length = len(label)\n",
        "        if len(seq_tensor) != length or len(seq_lengths) != length:\n",
        "            print(\"The lengths doesn't match with each other\")\n",
        "            print(\"seq_tensor:\", len(seq_tensor))\n",
        "            print(\"seq_length:\", len(seq_lengths))\n",
        "            print(\"label:\", length)\n",
        "            return False\n",
        "          \n",
        "        return True\n",
        "    \n",
        "    def shuffle(self, seq_tensor, seq_lengths, label):\n",
        "        shuffled_idx = torch.randperm(label.shape[0])\n",
        "        seq_tensor = seq_tensor[shuffled_idx]\n",
        "        seq_lenghts = seq_lengths[shuffled_idx]\n",
        "        label = label[shuffled_idx]\n",
        "        return seq_tensor, seq_lengths, label\n",
        "        \n",
        "    def divide_train_valid_test(self, seq_tensor, seq_lengths, label, PCT_TRAIN = 0.7, PCT_VALID = 0.2, do_shuffle= True):\n",
        "        '''\n",
        "        PCT_TRAIN: the percent of train set\n",
        "        PCT_VALID: the percent of validation set\n",
        "        The rest part will be the test set\n",
        "        '''\n",
        "        assert self.check_length(seq_tensor, seq_lengths, label)\n",
        "        \n",
        "        length = len(label)\n",
        "        \n",
        "        if do_shuffle:\n",
        "            seq_tensor, seq_lengths, label = self.shuffle(seq_tensor, seq_lengths, label)\n",
        "\n",
        "        train_seq_tensor = seq_tensor[:int(length*PCT_TRAIN)] \n",
        "        train_seq_lengths = seq_lengths[:int(length*PCT_TRAIN)]\n",
        "        train_label = label[:int(length*PCT_TRAIN)]\n",
        "\n",
        "        valid_seq_tensor = seq_tensor[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
        "        valid_seq_lengths = seq_lengths[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
        "        valid_label = label[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))]\n",
        "\n",
        "        test_seq_tensor = seq_tensor[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "        test_seq_lengths = seq_lengths[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "        test_label = label[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "\n",
        "        print(\"train:\", train_seq_tensor.shape)\n",
        "        print(\"valid:\", valid_seq_tensor.shape)\n",
        "        print(\"test:\", test_seq_tensor.shape)\n",
        "        \n",
        "        return train_seq_tensor, train_seq_lengths, train_label, \\\n",
        "                valid_seq_tensor, valid_seq_lengths, valid_label, \\\n",
        "                test_seq_tensor, test_seq_lengths, test_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9ADJtWW0bv0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "716fde31-399d-4c96-f509-7537847f0150"
      },
      "source": [
        "dd = DataDivider()\n",
        "train_seq_tensor, train_seq_lengths, train_label, \\\n",
        "valid_seq_tensor, valid_seq_lengths, valid_label, \\\n",
        "test_seq_tensor, test_seq_lengths, test_label = dd.divide_train_valid_test(seq_tensor, seq_lengths, label)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: torch.Size([23553, 38538])\n",
            "valid: torch.Size([6730, 38538])\n",
            "test: torch.Size([3365, 38538])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QN5DLcuBv-tf",
        "colab": {}
      },
      "source": [
        "import torch.utils.data.sampler as splr\n",
        "\n",
        "\n",
        "class CustomDataLoader(object):\n",
        "  def __init__(self, seq_tensor, seq_lengths, label_tensor, batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_tensor = seq_tensor\n",
        "    self.seq_lengths = seq_lengths\n",
        "    self.label_tensor = label_tensor\n",
        "    self.sampler = splr.BatchSampler(splr.RandomSampler(self.label_tensor), self.batch_size, False)\n",
        "    self.sampler_iter = iter(self.sampler)\n",
        "    \n",
        "  def __iter__(self):\n",
        "    self.sampler_iter = iter(self.sampler) # reset sampler iterator\n",
        "    return self\n",
        "\n",
        "  def _next_index(self):\n",
        "    return next(self.sampler_iter) # may raise StopIteration\n",
        "\n",
        "  def __next__(self):\n",
        "    index = self._next_index()\n",
        "\n",
        "    subset_seq_tensor = self.seq_tensor[index]\n",
        "    subset_seq_lengths = self.seq_lengths[index]\n",
        "    subset_label_tensor = self.label_tensor[index]\n",
        "\n",
        "    subset_seq_lengths, perm_idx = subset_seq_lengths.sort(0, descending=True)\n",
        "    subset_seq_tensor = subset_seq_tensor[perm_idx]\n",
        "    subset_label_tensor = subset_label_tensor[perm_idx]\n",
        "\n",
        "    return subset_seq_tensor, subset_seq_lengths, subset_label_tensor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sampler)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ol-GoapHYvE",
        "colab": {}
      },
      "source": [
        "batch_size = 200\n",
        "train_loader = CustomDataLoader(train_seq_tensor, train_seq_lengths, train_label, batch_size)\n",
        "valid_loader = CustomDataLoader(valid_seq_tensor, valid_seq_lengths, valid_label, batch_size)\n",
        "test_loader = CustomDataLoader(test_seq_tensor, test_seq_lengths, test_label, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NgU1rRGlHYvO",
        "colab": {}
      },
      "source": [
        "# Define Model\n",
        "'''\n",
        "1) Embedding Layer\n",
        "2) LSTM\n",
        "3) Fully Connected Layer\n",
        "4) Sigmoid Activation\n",
        "'''\n",
        "\n",
        "DEBUG = False\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class SpamHamLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers,\\\n",
        "                 drop_out_in_lstm, drop_out, output_size, device):\n",
        "\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_out_in_lstm, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, seq_lengths):\n",
        "\n",
        "        # embeddings\n",
        "        embedded_seq_tensor = self.embedding(x)\n",
        "        if DEBUG:\n",
        "          print(\"embedded_seq_tensor = self.embedding(x)\", embedded_seq_tensor.shape)\n",
        "                \n",
        "        # pack, remove pads\n",
        "        packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\n",
        "        if DEBUG:\n",
        "          print(\"packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\")\n",
        "          print(packed_input.data.shape)\n",
        "          print(packed_input.batch_sizes.shape)\n",
        "        \n",
        "        # lstm\n",
        "        packed_output, (ht, ct) = self.lstm(packed_input, None)\n",
        "        if DEBUG:\n",
        "          print(\"packed_output, (ht, ct) = self.lstm(packed_input, None)\")\n",
        "          print(packed_output.data.shape)\n",
        "          print(packed_output.batch_sizes.shape)\n",
        "          print(\"ht\")\n",
        "          print(ht.shape)\n",
        "        \n",
        "        # unpack, recover padded sequence\n",
        "        output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
        "        # output : batch_size X max_seq_len X hidden_dim\n",
        "        if DEBUG:\n",
        "          print(\"output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\")\n",
        "          print(output.shape)\n",
        "          print(input_sizes)\n",
        "       \n",
        "        # gather the last output in each batch\n",
        "        last_idxs = (input_sizes - 1).to(self.device) # last_idxs = input_sizes - torch.ones_like(input_sizes)\n",
        "        output = torch.gather(output, 1, last_idxs.view(-1, 1).unsqueeze(2).repeat(1, 1, self.hidden_dim)).squeeze() # [batch_size, hidden_dim]\n",
        "        if DEBUG:\n",
        "          print(output.shape) \n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc(output).squeeze()\n",
        "        if DEBUG:\n",
        "          print(\"output = self.fc(output)\", output.shape)\n",
        "               \n",
        "        # sigmoid function\n",
        "        output = self.sig(output)\n",
        "        \n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3doWR825pMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class Model_wrapper(object):\n",
        "  \n",
        "\tdef set_params(self, vocab_size, \\\n",
        "\t\t\t\t\t   embedding_dim = 100, \\\n",
        "\t\t\t\t\t   hidden_dim = 15, \\\n",
        "\t\t\t\t\t   n_layers = 2, \\\n",
        "\t\t\t\t\t   drop_out_in_lstm = 0.2, \\\n",
        "\t\t\t\t\t   drop_out = 0.2, \\\n",
        "\t\t\t\t\t   output_size = 1, \\\n",
        "\t\t\t\t\t   train_on_gpu = True):\n",
        "    \n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_dim = embedding_dim\n",
        "\t\tself.hidden_dim = hidden_dim\n",
        "\t\tself.n_layers = 2\n",
        "\t\tself.drop_out_in_lstm = drop_out_in_lstm\n",
        "\t\tself.drop_out = drop_out\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.train_on_gpu = train_on_gpu\n",
        "\t\tself.device = \"cuda\" if torch.cuda.is_available() and train_on_gpu else \"cpu\" \n",
        "\n",
        "\tdef set_model(self, do_print = True):\n",
        "\t\tself.model = SpamHamLSTM(self.vocab_size, self.embedding_dim, self.hidden_dim, self.n_layers, \\\n",
        "\t\t\t\t\t self.drop_out_in_lstm, self.drop_out, self.output_size, self.device)\n",
        "\t\tself.model = self.model.to(self.device)\n",
        "\t\tif do_print:\n",
        "\t\t\tprint(self.model)\n",
        "\n",
        "\tdef train(self, train_loader, valid_loader, criterion = \"default\", optimizer=\"default\", learning_rate = 0.03, use_scheduler = True, \\\n",
        "         epochs = 6, validate_every = 10, gradient_clip = 5):\n",
        "\n",
        "\t\tif criterion == \"default\" :\n",
        "\t\t\tcriterion = nn.BCELoss()\n",
        "\t\tprint(criterion)\n",
        "     \n",
        "\n",
        "\t\tif optimizer == \"default\" :\n",
        "\t\t\toptimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "\t\tprint(optimizer)\n",
        "\n",
        "\t\tif use_scheduler :\n",
        "\t\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.5, patience = 2)\n",
        "\n",
        "\t\tcounter = 0\n",
        "\n",
        "\t\tself.model.train()\n",
        "\t\n",
        "\t\tval_losses = []\n",
        "\t\tval_min_loss = 1000000\n",
        "\n",
        "\t\tfor e in range(epochs):\n",
        "\n",
        "\t\t\tif use_scheduler :\n",
        "\t\t\t\tscheduler.step(e)\n",
        "\n",
        "\t\t\tfor seq_tensor, seq_tensor_lengths, label in iter(train_loader):\n",
        "\t\t\t\tcounter += 1\n",
        "\n",
        "\t\t\t\tseq_tensor = seq_tensor.to(self.device)\n",
        "\t\t\t\tseq_tensor_lengths = seq_tensor_lengths.to(self.device)\n",
        "\t\t\t\tlabel = label.to(self.device)\n",
        "\n",
        "\t\t\t\t# get the output from the model\n",
        "\t\t\t\toutput = self.model(seq_tensor, seq_tensor_lengths)\n",
        "\n",
        "\t\t\t\t# calculate the loss and perform backprop\n",
        "\t\t\t\tloss = criterion(output, label.float())\n",
        "\t\t\t\toptimizer.zero_grad() \n",
        "\t\t\t\tloss.backward()\n",
        "\n",
        "\t\t\t\t# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "\t\t\t\tnn.utils.clip_grad_norm_(net.parameters(), gradient_clip)\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\t# loss stats\n",
        "\t\t\t\tif counter % validate_every == 0:\n",
        "\t\t\t\t\t# Get validation loss\n",
        "\t\t\t\t\tval_losses_in_itr = []\n",
        "\t\t\t\t\tsums = []\n",
        "\t\t\t\t\tsizes = []\n",
        "\n",
        "\t\t\t\t\tself.model.eval()\n",
        "\n",
        "\t\t\t\t\tfor seq_tensor, seq_tensor_lengths, label in iter(valid_loader):\n",
        "\n",
        "\t\t\t\t\t\tseq_tensor = seq_tensor.to(device)\n",
        "\t\t\t\t\t\tseq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "\t\t\t\t\t\tlabel = label.to(device)\n",
        "\t\t\t\t\t\toutput = self.model(seq_tensor, seq_tensor_lengths)\n",
        "\n",
        "\t\t\t\t\t\t# losses\n",
        "\t\t\t\t\t\tval_loss = criterion(output, label.float())     \n",
        "\t\t\t\t\t\tval_losses_in_itr.append(val_loss.item())\n",
        "\n",
        "\t\t\t\t\t\t# accuracy\n",
        "\t\t\t\t\t\tbinary_output = (output >= 0.5).short() # short(): torch.int16\n",
        "\t\t\t\t\t\tright_or_not = torch.eq(binary_output, label)\n",
        "\t\t\t\t\t\tsums.append(torch.sum(right_or_not).float().item())\n",
        "\t\t\t\t\t\tsizes.append(right_or_not.shape[0])\n",
        "\n",
        "\t\t\t\t\tval_losses.append(np.mean(val_losses_in_itr))\n",
        "\t\t\t\t\tif val_min_loss > val_losses[-1]:\n",
        "\t\t\t\t\t\tval_min_loss = val_losses[-1]\n",
        "\t\t\t\t\t\tself.save_state_dict('./', 'lstm_model_saved_at_{}.pth'.format(counter))\n",
        "\n",
        "\t\t\t\t\taccuracy = np.sum(sums) / np.sum(sizes)\n",
        "\n",
        "\t\t\t\t\tself.model.train()\n",
        "\t\t\t\t\tprint(\"Epoch: {:2d}/{:2d}\\t\".format(e+1, epochs),\n",
        "\t\t\t\t\t\t  \"Steps: {:3d}\\t\".format(counter),\n",
        "\t\t\t\t\t\t  \"Loss: {:.5f}\\t\".format(loss.item()),\n",
        "\t\t\t\t\t\t  \"Val Loss: {:.5f}\\t\".format(np.mean(val_losses_in_itr)),\n",
        "\t\t\t\t\t\t  \"Accuracy: {:.3f}\".format(accuracy))    \n",
        "\n",
        "\tdef test(self, test_loader, criterion = \"default\"):\n",
        "\t\tif criterion == \"default\":\n",
        "\t\t\tcriterion = nn.BCELoss()\n",
        "\n",
        "\t\ttest_losses = []\n",
        "\t\tsums = []\n",
        "\t\tsizes = []\n",
        "\n",
        "\t\tself.model.eval()\n",
        "\n",
        "\t\ttest_losses = []\n",
        "\t\t\n",
        "\t\tfor seq_tensor, seq_tensor_lengths, label in iter(test_loader):\n",
        "\t\t\tseq_tensor = seq_tensor.to(self.device)\n",
        "\t\t\tseq_tensor_lengths = seq_tensor_lengths.to(self.device)\n",
        "\t\t\tlabel = label.to(self.device)\n",
        "\t\t\toutput = self.model(seq_tensor, seq_tensor_lengths)\n",
        "\n",
        "\t\t\t# losses\n",
        "\t\t\ttest_loss = criterion(output, label.float())     \n",
        "\t\t\ttest_losses.append(test_loss.item())\n",
        "\n",
        "\t\t\t# accuracy\n",
        "\t\t\tbinary_output = (output >= 0.5).short() # short(): torch.int16\n",
        "\t\t\tright_or_not = torch.eq(binary_output, label)\n",
        "\t\t\tsums.append(torch.sum(right_or_not).float().item())\n",
        "\t\t\tsizes.append(right_or_not.shape[0])\n",
        "\n",
        "\t\taccuracy = np.sum(sums) / np.sum(sizes)\n",
        "\t\tprint(\"Test Loss: {:.6f}\\t\".format(np.mean(test_losses)),\n",
        "\t\t\"Accuracy: {:.3f}\".format(accuracy))\n",
        "    \n",
        "\tdef load_state_dict(self, saved_dir='./', file_name='saved_model.pth', do_print = True):\n",
        "\t\toutput_path = os.path.join(saved_dir, file_name)\n",
        "\t\tcheckpoint = torch.load(output_path)\n",
        "\t\tstate_dict = checkpoint['net']\n",
        "\t\tself.model.load_state_dict(state_dict)\n",
        "\t\tif do_print:\n",
        "\t\t\tfor name, param in self.model.named_parameters():\n",
        "\t\t\t\tif param.requires_grad:\n",
        "\t\t\t\t\tprint(name, param.data.shape)\n",
        "\n",
        "\tdef save_state_dict(self, saved_dir='./', file_name='saved_model.pth', do_print = True):\n",
        "\t\tos.makedirs(saved_dir, exist_ok=True)\n",
        "\t\tcheck_point = {\n",
        "\t\t\t'net': self.model.state_dict()\n",
        "\t\t}\n",
        "\t\toutput_path = os.path.join(saved_dir, file_name)\n",
        "\t\ttorch.save(check_point, output_path)\n",
        "\t\tif do_print:\n",
        "\t\t\tprint(\"saved as\", output_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FFjoOB0mfq9v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "109f0869-07be-496d-86b6-d88b71594c3f"
      },
      "source": [
        "vocab_size = len(vocab_to_int)\n",
        "print(vocab_size)\n",
        "mw = Model_wrapper()\n",
        "mw.set_params(vocab_size)\n",
        "mw.set_model()"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "159198\n",
            "SpamHamLSTM(\n",
            "  (embedding): Embedding(159198, 100)\n",
            "  (lstm): LSTM(100, 15, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.2)\n",
            "  (fc): Linear(in_features=15, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BugCrW2bytwI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d08082d5-d858-4fbc-f6c3-b85b312adffa"
      },
      "source": [
        "mw.train(train_loader, valid_loader)"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BCELoss()\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.03\n",
            "    weight_decay: 0\n",
            ")\n",
            "saved as ./lstm_model_saved_at_10.pth\n",
            "Epoch:  1/ 6\t Steps:  10\t Loss: 0.69778\t Val Loss: 0.68049\t Accuracy: 0.558\n",
            "Epoch:  1/ 6\t Steps:  20\t Loss: 0.68155\t Val Loss: 0.69174\t Accuracy: 0.562\n",
            "Epoch:  1/ 6\t Steps:  30\t Loss: 0.86717\t Val Loss: 0.96266\t Accuracy: 0.654\n",
            "saved as ./lstm_model_saved_at_40.pth\n",
            "Epoch:  1/ 6\t Steps:  40\t Loss: 0.65479\t Val Loss: 0.63954\t Accuracy: 0.675\n",
            "saved as ./lstm_model_saved_at_50.pth\n",
            "Epoch:  1/ 6\t Steps:  50\t Loss: 0.43597\t Val Loss: 0.40465\t Accuracy: 0.881\n",
            "saved as ./lstm_model_saved_at_60.pth\n",
            "Epoch:  1/ 6\t Steps:  60\t Loss: 0.43143\t Val Loss: 0.37936\t Accuracy: 0.858\n",
            "saved as ./lstm_model_saved_at_70.pth\n",
            "Epoch:  1/ 6\t Steps:  70\t Loss: 0.37240\t Val Loss: 0.35781\t Accuracy: 0.888\n",
            "saved as ./lstm_model_saved_at_80.pth\n",
            "Epoch:  1/ 6\t Steps:  80\t Loss: 0.19122\t Val Loss: 0.22209\t Accuracy: 0.929\n",
            "Epoch:  1/ 6\t Steps:  90\t Loss: 0.22073\t Val Loss: 0.22785\t Accuracy: 0.923\n",
            "Epoch:  1/ 6\t Steps: 100\t Loss: 0.23114\t Val Loss: 0.24574\t Accuracy: 0.916\n",
            "saved as ./lstm_model_saved_at_110.pth\n",
            "Epoch:  1/ 6\t Steps: 110\t Loss: 0.25611\t Val Loss: 0.20789\t Accuracy: 0.938\n",
            "saved as ./lstm_model_saved_at_120.pth\n",
            "Epoch:  2/ 6\t Steps: 120\t Loss: 0.19812\t Val Loss: 0.17534\t Accuracy: 0.938\n",
            "Epoch:  2/ 6\t Steps: 130\t Loss: 0.50914\t Val Loss: 0.39535\t Accuracy: 0.860\n",
            "Epoch:  2/ 6\t Steps: 140\t Loss: 0.51177\t Val Loss: 0.53066\t Accuracy: 0.770\n",
            "Epoch:  2/ 6\t Steps: 150\t Loss: 0.37933\t Val Loss: 0.29201\t Accuracy: 0.893\n",
            "Epoch:  2/ 6\t Steps: 160\t Loss: 0.22253\t Val Loss: 0.19491\t Accuracy: 0.933\n",
            "saved as ./lstm_model_saved_at_170.pth\n",
            "Epoch:  2/ 6\t Steps: 170\t Loss: 0.20248\t Val Loss: 0.13878\t Accuracy: 0.952\n",
            "Epoch:  2/ 6\t Steps: 180\t Loss: 0.20431\t Val Loss: 0.14752\t Accuracy: 0.956\n",
            "saved as ./lstm_model_saved_at_190.pth\n",
            "Epoch:  2/ 6\t Steps: 190\t Loss: 0.09283\t Val Loss: 0.12650\t Accuracy: 0.960\n",
            "saved as ./lstm_model_saved_at_200.pth\n",
            "Epoch:  2/ 6\t Steps: 200\t Loss: 0.16099\t Val Loss: 0.11573\t Accuracy: 0.965\n",
            "Epoch:  2/ 6\t Steps: 210\t Loss: 0.15949\t Val Loss: 0.13193\t Accuracy: 0.959\n",
            "Epoch:  2/ 6\t Steps: 220\t Loss: 0.15652\t Val Loss: 0.13314\t Accuracy: 0.949\n",
            "Epoch:  2/ 6\t Steps: 230\t Loss: 0.10744\t Val Loss: 0.11938\t Accuracy: 0.956\n",
            "saved as ./lstm_model_saved_at_240.pth\n",
            "Epoch:  3/ 6\t Steps: 240\t Loss: 0.06447\t Val Loss: 0.10325\t Accuracy: 0.966\n",
            "saved as ./lstm_model_saved_at_250.pth\n",
            "Epoch:  3/ 6\t Steps: 250\t Loss: 0.08209\t Val Loss: 0.09317\t Accuracy: 0.972\n",
            "Epoch:  3/ 6\t Steps: 260\t Loss: 0.08789\t Val Loss: 0.10065\t Accuracy: 0.970\n",
            "Epoch:  3/ 6\t Steps: 270\t Loss: 0.08109\t Val Loss: 0.09580\t Accuracy: 0.971\n",
            "Epoch:  3/ 6\t Steps: 280\t Loss: 0.06278\t Val Loss: 0.10186\t Accuracy: 0.967\n",
            "Epoch:  3/ 6\t Steps: 290\t Loss: 0.06593\t Val Loss: 0.10351\t Accuracy: 0.963\n",
            "Epoch:  3/ 6\t Steps: 300\t Loss: 0.08569\t Val Loss: 0.09559\t Accuracy: 0.969\n",
            "saved as ./lstm_model_saved_at_310.pth\n",
            "Epoch:  3/ 6\t Steps: 310\t Loss: 0.08519\t Val Loss: 0.08957\t Accuracy: 0.974\n",
            "saved as ./lstm_model_saved_at_320.pth\n",
            "Epoch:  3/ 6\t Steps: 320\t Loss: 0.11239\t Val Loss: 0.08195\t Accuracy: 0.974\n",
            "Epoch:  3/ 6\t Steps: 330\t Loss: 0.08303\t Val Loss: 0.08518\t Accuracy: 0.972\n",
            "saved as ./lstm_model_saved_at_340.pth\n",
            "Epoch:  3/ 6\t Steps: 340\t Loss: 0.05918\t Val Loss: 0.07716\t Accuracy: 0.975\n",
            "Epoch:  3/ 6\t Steps: 350\t Loss: 0.06174\t Val Loss: 0.08333\t Accuracy: 0.975\n",
            "Epoch:  4/ 6\t Steps: 360\t Loss: 0.01857\t Val Loss: 0.07771\t Accuracy: 0.977\n",
            "saved as ./lstm_model_saved_at_370.pth\n",
            "Epoch:  4/ 6\t Steps: 370\t Loss: 0.04995\t Val Loss: 0.07380\t Accuracy: 0.977\n",
            "Epoch:  4/ 6\t Steps: 380\t Loss: 0.02744\t Val Loss: 0.07456\t Accuracy: 0.978\n",
            "saved as ./lstm_model_saved_at_390.pth\n",
            "Epoch:  4/ 6\t Steps: 390\t Loss: 0.02165\t Val Loss: 0.07148\t Accuracy: 0.978\n",
            "Epoch:  4/ 6\t Steps: 400\t Loss: 0.03294\t Val Loss: 0.07224\t Accuracy: 0.979\n",
            "saved as ./lstm_model_saved_at_410.pth\n",
            "Epoch:  4/ 6\t Steps: 410\t Loss: 0.04498\t Val Loss: 0.06640\t Accuracy: 0.977\n",
            "Epoch:  4/ 6\t Steps: 420\t Loss: 0.05422\t Val Loss: 0.06789\t Accuracy: 0.977\n",
            "saved as ./lstm_model_saved_at_430.pth\n",
            "Epoch:  4/ 6\t Steps: 430\t Loss: 0.11442\t Val Loss: 0.06456\t Accuracy: 0.977\n",
            "saved as ./lstm_model_saved_at_440.pth\n",
            "Epoch:  4/ 6\t Steps: 440\t Loss: 0.04474\t Val Loss: 0.06260\t Accuracy: 0.979\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-180-716472d52336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-159-e85cc2a4b0c7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, valid_loader, criterion, optimizer, learning_rate, use_scheduler, epochs, validate_every, gradient_clip)\u001b[0m\n\u001b[1;32m     51\u001b[0m                                 \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                                 \u001b[0mseq_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                                 \u001b[0mseq_tensor_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_tensor_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1l8r0hKDDUL",
        "colab_type": "text"
      },
      "source": [
        "Interrupt the training, model at step 380 will be used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri_4O9ANEtvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2af00b97-1053-4f47-e68f-4e48e046aa40"
      },
      "source": [
        "mw.load_state_dict(saved_dir='./', file_name='lstm_model_saved_at_440.pth', do_print = False)\n",
        "mw.test(test_loader)"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.072314\t Accuracy: 0.977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oju9wsvLf_j_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c75ea75a-8274-4414-c1f1-bf9b1dfec65b"
      },
      "source": [
        "mw.load_state_dict(saved_dir='./', file_name='lstm_model_saved_at_310.pth', do_print = False)\n",
        "mw.test(test_loader)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.088483\t Accuracy: 0.976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8SudydcFsya",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "702551df-283e-45c6-8212-90eb6e8d24f7"
      },
      "source": [
        "mw.load_state_dict(saved_dir='./', file_name='lstm_model_saved_at_80.pth', do_print = False)\n",
        "mw.test(test_loader)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.214243\t Accuracy: 0.926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEHYXkKAGgJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Predictor(Model_wrapper):\n",
        "\tdef __init__(self, saved_dir='./', file_name = 'lstm_model_saved_at_380.pth'):\n",
        "\t\tvg = Vocab_to_int()\n",
        "\t\tself.vocab_to_int = vg.open_file()\n",
        "\t\tvocab_size = len(self.vocab_to_int)\n",
        "\t\tself.vr = Vectorizer(self.vocab_to_int)\n",
        "\t\tself.set_params(vocab_size, train_on_gpu = False)\n",
        "\t\tself.set_model(do_print = False)\n",
        "\t\tself.load_state_dict(saved_dir, file_name, do_print = False)\n",
        "\n",
        "\tdef predict(self, text, unnecessary = [\"-\", \".\", \",\", \"/\", \":\", \"@\", \"'\", \"!\"]):\t\n",
        "\t\ttext = text.lower()\n",
        "\t\ttext = ''.join([c for c in text if c not in unnecessary])\n",
        "\t\ttext = [text]\n",
        "\t\tseq_tensor, seq_tensor_lengths = self.vr.vectorize(text)\n",
        "\t\tseq_tensor_lengths = seq_tensor_lengths\n",
        "\t\tself.model.eval()\n",
        "\t\toutput = self.model(seq_tensor, seq_tensor_lengths)\n",
        "\t\treturn output.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cSxlzF2QRUS9",
        "outputId": "22aa08ca-f8e6-4e45-e4ce-eca0357b590f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "p = Predictor(file_name = 'lstm_model_saved_at_440.pth')\n",
        "\n",
        "myString = \"This is the greatest offer. You can't take this chance away! We offer the best product in the world\"\n",
        "result = p.predict(myString)\n",
        "print(result)\n",
        "\n",
        "myString = \"Hello, we have meeting with boss at 1:00 pm. Please prepare the document. I'll be there earlier, we need to discuss before the meeting\"\n",
        "result = p.predict(myString)\n",
        "print(result)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9464383721351624\n",
            "0.023171260952949524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z5vv1VofvMW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "edfa18f9-5cd6-49fc-cf82-3163a6d0b3d8"
      },
      "source": [
        "\n",
        "p = Predictor(file_name = 'lstm_model_saved_at_310.pth')\n",
        "\n",
        "myString = \"This is the greatest offer. You can't take this chance away! We offer the best product in the world\"\n",
        "result = p.predict(myString)\n",
        "print(result)\n",
        "\n",
        "myString = \"Hello, we have meeting with boss at 1:00 pm. Please prepare the document. I'll be there earlier, we need to discuss before the meeting\"\n",
        "result = p.predict(myString)\n",
        "print(result)"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9846702814102173\n",
            "0.003224465297535062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fJpXgRzduTX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "096b31ea-ba5e-4bd0-a918-dff94e00ae9a"
      },
      "source": [
        "\n",
        "p = Predictor(file_name = 'lstm_model_saved_at_80.pth')\n",
        "\n",
        "myString = \"This is the greatest offer. You can't take this chance away! We offer the best product in the world\"\n",
        "result = p.predict(myString)\n",
        "print(result)\n",
        "\n",
        "myString = \"Hello, we have meeting with boss at 1:00 pm. Please prepare the document. I'll be there earlier, we need to discuss before the meeting\"\n",
        "result = p.predict(myString)\n",
        "print(result)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9792095422744751\n",
            "0.03566703572869301\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}