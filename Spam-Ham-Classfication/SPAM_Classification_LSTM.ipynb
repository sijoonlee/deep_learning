{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "SPAM Classification LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW8XHZIlHxG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!tar -xopf eron.tar\n",
        "!tar -zxvf data.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J3UlzLCU9Hj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "class FileReader(object):\n",
        "  def __init__(self):\n",
        "    self.ham = []\n",
        "    self.spam = []\n",
        "    self.ham_paths = [\"enron1/ham/*.txt\", \"enron2/ham/*.txt\", \"enron3/ham/*.txt\", \"enron4/ham/*.txt\", \"enron5/ham/*.txt\", \"enron6/ham/*.txt\"]\n",
        "    self.spam_paths = [\"enron1/spam/*.txt\", \"enron2/spam/*.txt\", \"enron3/spam/*.txt\", \"enron4/spam/*.txt\", \"enron5/spam/*.txt\", \"enron6/spam/*.txt\"]\n",
        "  \n",
        "  def read_file(self, path, minimum_word_count = 3, unnecessary =  [\"-\", \".\", \",\", \"/\", \":\", \"@\"]):\n",
        "    files  = glob.glob(path)\n",
        "    content_list = []\n",
        "    for file in files:\n",
        "        with open(file, encoding=\"ISO-8859-1\") as f:\n",
        "            content = f.read()\n",
        "            if len(content.split()) > minimum_word_count:      \n",
        "              content = content.lower()\n",
        "              if len(unnecessary) is not 0:\n",
        "                  content = ''.join([c for c in content if c not in unnecessary])\n",
        "              content_list.append(content)\n",
        "    return content_list\n",
        "  \n",
        "  def truncate_before_combine(self, data, maximum_length = 5000):\n",
        "    if maximum_length is not 0:\n",
        "      if len(data) > maximum_length:\n",
        "        random.shuffle(data)\n",
        "        data = data[:maximum_length]\n",
        "    return data\n",
        "  \n",
        "  def load_ham_and_spam(self, ham_paths = \"default\", spam_paths = \"default\", truncation_length = 5000): # 0 for no truncation\n",
        "    \n",
        "    if ham_paths == \"default\":\n",
        "      ham_paths = self.ham_paths\n",
        "    if spam_paths == \"default\":\n",
        "      spam_paths = self.spam_paths\n",
        "    \n",
        "    self.ham = [ item for path in ham_paths for item in self.read_file(path) ]\n",
        "    if truncation_length != 0:\n",
        "      self.ham = self.truncate_before_combine(self.ham, truncation_length)\n",
        "    print(\"ham length \", len(self.ham))\n",
        "    \n",
        "    self.spam = [item for path in spam_paths for item in self.read_file(path) ]\n",
        "    if truncation_length != 0:\n",
        "      self.spam = self.truncate_before_combine(self.spam, truncation_length)\n",
        "    print(\"spam length \", len(self.spam))\n",
        "    \n",
        "    data = self.ham + self.spam\n",
        "    \n",
        "    ham_label = [0 for _ in range(len(self.ham))]\n",
        "    spam_label = [1 for _ in range(len(self.spam))]\n",
        "    \n",
        "    label_tensor = torch.as_tensor(ham_label + spam_label, dtype = torch.int16)\n",
        "    \n",
        "    return data, label_tensor\n",
        "  \n",
        "  def print_sample(self, which =\"both\"): # ham, spam or both\n",
        "    if which == \"ham\" or which == \"both\":\n",
        "      idx = random.randint(0, len(self.ham))\n",
        "      print(\"----------- ham sample -------------\")\n",
        "      print(self.ham[idx])\n",
        "    if which == \"spam\" or which == \"both\":\n",
        "      idx = random.randint(0, len(self.spam))\n",
        "      print(\"----------- spam sample -------------\")\n",
        "      print(self.spam[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j3m0dUddcgP",
        "colab_type": "code",
        "outputId": "7e95fbb7-c8da-4c93-d658-4359d011356b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "reader = FileReader()\n",
        "\n",
        "data, label = reader.load_ham_and_spam(\"default\", \"default\", truncation_length = 0)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ham length  16540\n",
            "spam length  17108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg5nJxOMxVQW",
        "colab_type": "code",
        "outputId": "c0a5b34f-9c0f-4842-e200-00625e3aef6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "reader.print_sample()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------- ham sample -------------\n",
            "subject tw outage\n",
            "operations is digging out 2000 feet of pipe to begin the hydro test today or\n",
            "thursday  if the test results are good  they ' ll recoat the pipe and put it\n",
            "back in service  we could be at 80 % volume on friday afternoon  but this is\n",
            "tentative \n",
            "also  the smart pig test run several months ago identified 4 potential areas\n",
            "that need inspections west of station 5  while east of thoreau is down \n",
            "these 4 areas will also be inspected \n",
            "do not pass this info onto customers at this point till we have further\n",
            "information  we will make a posting hopefully this afternoon  operations is\n",
            "also trying to combine some additional compressor work to this outage ( i  e \n",
            "grouting another unit and replacing some valves )  this work will limit our\n",
            "west deliveries to 875  000  this notice will also be posted shortly \n",
            "see me if you have questions\n",
            "kh\n",
            "----------- spam sample -------------\n",
            "subject paliourg no pre scription f e e s\n",
            "your easy  to  use r x solution is here\n",
            "totally f r e e prescriptions ! !\n",
            "no prior prescription needed !\n",
            "we believe ordering medication should be\n",
            "as simple as ordering anything else on the internet \n",
            "private  secure  and easy \n",
            "no prescription required  no long lengthy forms to fill out \n",
            "your easy  to  use solution is here \n",
            "thank you for your time \n",
            "eloy robles\n",
            "paliourg below is for u if u dislike e  commerce\n",
            "http    paliourg  allcres 4  com  off  html\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3JqHhaQcWmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "class Vocab_to_int(object):\n",
        "\n",
        "  def __init__(self, saved_dir='./', file_name=\"vocab_to_int.csv\"):\n",
        "      os.makedirs(saved_dir, exist_ok=True)\n",
        "      self.path = os.path.join(saved_dir, file_name)\n",
        "\n",
        "  def save_file(self, vocab_to_int):\n",
        "      df = pd.DataFrame(list(vocab_to_int.items()))\n",
        "      df.dropna(inplace=True)\n",
        "      df = df.T\n",
        "      df.to_csv(self.path, index=False, header=False)\n",
        "\n",
        "  def open_file(self, path = \"Default\"):\n",
        "      if path == \"Default\":\n",
        "        path = self.path\n",
        "      df = pd.read_csv(path)\n",
        "      df.dropna(inplace=True)\n",
        "      dict = df.to_dict('records')[0]\n",
        "      return dict\n",
        "\n",
        "  def generate(self, seqs, save_file=True):\n",
        "      vocabs = [vocab for seq in seqs for vocab in seq.split()]\n",
        "      # a = [  word for seq in [\"a d\",\"b d\",\"c d\"] for word in seq.split() ]\n",
        "      # ['a', 'd', 'b', 'd', 'c', 'd']\n",
        "\n",
        "      # Count word frequency\n",
        "      # Counter({'the': 39770, 'to': 32356, 'and': 22835, 'of': 19607, 'a': 17100, '_': 16955, 'you': 15593, 'in': 14481, .....\n",
        "      vocab_count = Counter(vocabs)\n",
        "\n",
        "      vocab_count = vocab_count.most_common(len(vocab_count))\n",
        "\n",
        "      vocab_to_int = {word : index+1 for index, (word, count) in enumerate(vocab_count)}\n",
        "      vocab_to_int.update({'__PADDING__': 0}) # index 0 for padding\n",
        "\n",
        "      if save_file:\n",
        "        self.save_file(vocab_to_int)\n",
        "\n",
        "      return vocab_to_int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcyiNGm3cZHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vti = Vocab_to_int()\n",
        "vocab_to_int = vti.generate(data, save_file=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Vdi2zuKHYue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "arg: ham or spam data (numpy array)\n",
        "return: int dictionary [ word_n: count_n, ... ]\n",
        "'''\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class Vectorizer(object):\n",
        "  \n",
        "  def __init__(self, vocab_to_int):\n",
        "    self.vocab_to_int = vocab_to_int\n",
        "    \n",
        "  def vectorize_seqs(self, seqs):\n",
        "    # Vectorize each sequence\n",
        "    vectorized_seqs = []\n",
        "    for seq in seqs: \n",
        "      vectorized_seqs.append([self.vocab_to_int[word] for word in seq.split()])\n",
        "    return vectorized_seqs\n",
        "  \n",
        "  def add_padding(self, vectorized_seqs, seq_lengths):\n",
        "    seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
        "    for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "      seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "    return seq_tensor\n",
        "  \n",
        "  def vectorize(self, seqs):\n",
        "    vectorized_seqs = self.vectorize_seqs(seqs)\n",
        "    seq_lengths = torch.LongTensor(list(map(len, vectorized_seqs)))\n",
        "    seq_tensor = self.add_padding(vectorized_seqs, seq_lengths)\n",
        "    \n",
        "    return seq_tensor, seq_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA1eZE5mouSo",
        "colab_type": "code",
        "outputId": "84da4e4d-1736-41a8-93e8-b8f24f545503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(vocab_to_int))\n",
        "v = Vectorizer(vocab_to_int)\n",
        "seq_tensor, seq_lengths = v.vectorize(data)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "159199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN5DLcuBv-tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.utils.data.sampler as splr\n",
        "\n",
        "\n",
        "class CustomDataLoader(object):\n",
        "  def __init__(self, seq_tensor, seq_lengths, label_tensor, batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_tensor = seq_tensor\n",
        "    self.seq_lengths = seq_lengths\n",
        "    self.label_tensor = label_tensor\n",
        "    self.sampler = splr.BatchSampler(splr.RandomSampler(self.label_tensor), self.batch_size, False)\n",
        "    self.sampler_iter = iter(self.sampler)\n",
        "    \n",
        "  def __iter__(self):\n",
        "    self.sampler_iter = iter(self.sampler) # reset sampler iterator\n",
        "    return self\n",
        "\n",
        "  def _next_index(self):\n",
        "    return next(self.sampler_iter) # may raise StopIteration\n",
        "\n",
        "  def __next__(self):\n",
        "    index = self._next_index()\n",
        "\n",
        "    subset_seq_tensor = self.seq_tensor[index]\n",
        "    subset_seq_lengths = self.seq_lengths[index]\n",
        "    subset_label_tensor = self.label_tensor[index]\n",
        "\n",
        "    subset_seq_lengths, perm_idx = subset_seq_lengths.sort(0, descending=True)\n",
        "    subset_seq_tensor = subset_seq_tensor[perm_idx]\n",
        "    subset_label_tensor = subset_label_tensor[perm_idx]\n",
        "\n",
        "    return subset_seq_tensor, subset_seq_lengths, subset_label_tensor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sampler)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT6HTGm3HYu7",
        "colab_type": "code",
        "outputId": "5f29504a-95e7-44fa-a934-561a36d60e54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "shuffled_idx = torch.randperm(label.shape[0])\n",
        "\n",
        "seq_tensor = seq_tensor[shuffled_idx]\n",
        "seq_lenghts = seq_lengths[shuffled_idx]\n",
        "label = label[shuffled_idx]\n",
        "\n",
        "PCT_TRAIN = 0.7\n",
        "PCT_VALID = 0.2\n",
        "\n",
        "length = len(label)\n",
        "train_seq_tensor = seq_tensor[:int(length*PCT_TRAIN)] \n",
        "train_seq_lengths = seq_lengths[:int(length*PCT_TRAIN)]\n",
        "train_label = label[:int(length*PCT_TRAIN)]\n",
        "\n",
        "valid_seq_tensor = seq_tensor[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
        "valid_seq_lengths = seq_lengths[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
        "valid_label = label[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))]\n",
        "\n",
        "test_seq_tensor = seq_tensor[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "test_seq_lengths = seq_lengths[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "test_label = label[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "\n",
        "print(train_seq_tensor.shape)\n",
        "print(valid_seq_tensor.shape)\n",
        "print(test_seq_tensor.shape)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([23553, 39470])\n",
            "torch.Size([6730, 39470])\n",
            "torch.Size([3365, 39470])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ol-GoapHYvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set shuffle = False since data is already shuffled\n",
        "batch_size = 200\n",
        "train_loader = CustomDataLoader(train_seq_tensor, train_seq_lengths, train_label, batch_size)\n",
        "valid_loader = CustomDataLoader(valid_seq_tensor, valid_seq_lengths, valid_label, batch_size)\n",
        "test_loader = CustomDataLoader(test_seq_tensor, test_seq_lengths, test_label, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgU1rRGlHYvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Model\n",
        "'''\n",
        "1) Embedding Layer\n",
        "2) LSTM\n",
        "3) Fully Connected Layer\n",
        "4) Sigmoid Activation\n",
        "'''\n",
        "\n",
        "DEBUG = False\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class SpamHamLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers,\\\n",
        "                 drop_lstm=0.1, drop_out = 0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_lstm, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, seq_lengths):\n",
        "\n",
        "        # embeddings\n",
        "        embedded_seq_tensor = self.embedding(x)\n",
        "        if DEBUG:\n",
        "          print(\"embedded_seq_tensor = self.embedding(x)\", embedded_seq_tensor.shape)\n",
        "                \n",
        "        # pack, remove pads\n",
        "        packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\n",
        "        if DEBUG:\n",
        "          print(\"packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\")\n",
        "          print(packed_input.data.shape)\n",
        "          print(packed_input.batch_sizes.shape)\n",
        "        \n",
        "        # lstm\n",
        "        packed_output, (ht, ct) = self.lstm(packed_input, None)\n",
        "        if DEBUG:\n",
        "          print(\"packed_output, (ht, ct) = self.lstm(packed_input, None)\")\n",
        "          print(packed_output.data.shape)\n",
        "          print(packed_output.batch_sizes.shape)\n",
        "          print(\"ht\")\n",
        "          print(ht.shape)\n",
        "        \n",
        "        # unpack, recover padded sequence\n",
        "        output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
        "        # output : batch_size X max_seq_len X hidden_dim\n",
        "        if DEBUG:\n",
        "          print(\"output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\")\n",
        "          print(output.shape)\n",
        "          print(input_sizes)\n",
        "       \n",
        "        # gather the last output in each batch\n",
        "        last_idxs = (input_sizes - 1).to(device) # last_idxs = input_sizes - torch.ones_like(input_sizes)\n",
        "        output = torch.gather(output, 1, last_idxs.view(-1, 1).unsqueeze(2).repeat(1, 1, self.hidden_dim)).squeeze() # [batch_size, hidden_dim]\n",
        "        if DEBUG:\n",
        "          print(output.shape) \n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc(output).squeeze()\n",
        "        if DEBUG:\n",
        "          print(\"output = self.fc(output)\", output.shape)\n",
        "               \n",
        "        # sigmoid function\n",
        "        output = self.sig(output)\n",
        "        \n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0JOX-wrHYvT",
        "colab_type": "code",
        "outputId": "faf22f67-f085-403c-bddb-7c9e65ddd928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "\n",
        "vocab_size = len(vocab_to_int)\n",
        "output_size = 1\n",
        "embedding_dim = 100 # int(vocab_size ** 0.25) # 15\n",
        "hidden_dim = 15\n",
        "n_layers = 2\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
        "net = SpamHamLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, \\\n",
        "                 0.2, 0.2)\n",
        "net = net.to(device)\n",
        "print(net)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SpamHamLSTM(\n",
            "  (embedding): Embedding(159199, 100)\n",
            "  (lstm): LSTM(100, 15, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.2)\n",
            "  (fc): Linear(in_features=15, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84V6GRYKUnFI",
        "colab_type": "code",
        "outputId": "6d651dc3-e943-4188-ebd6-5df0fa3916dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Debug Purpose - Test Run\n",
        "\n",
        "lr=0.0001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "net.eval()\n",
        "seq_tensor, seq_tensor_lengths, label = next(train_loader)\n",
        "\n",
        "seq_tensor = seq_tensor.to(device)\n",
        "seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "\n",
        "label = label.to(device)\n",
        "output = net(seq_tensor, seq_tensor_lengths)\n",
        "loss = criterion(output, label.float())\n",
        "\n",
        "binary_output = (output >= 0.5).short() # short(): torch.int16\n",
        "right_or_not = torch.eq(binary_output, label)\n",
        "accuracy = torch.sum(right_or_not).float().item()/right_or_not.shape[0]\n",
        "print(\"{:2.3f}\".format(accuracy*100))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51.500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFjoOB0mfq9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "def save_model(model, saved_dir='./', file_name='saved_model.pth'):\n",
        "    os.makedirs(saved_dir, exist_ok=True)\n",
        "    check_point = {\n",
        "        'net': model.state_dict()\n",
        "    }\n",
        "    output_path = os.path.join(saved_dir, file_name)\n",
        "    torch.save(check_point, output_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEgFa6BrPmRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss and optimization functions\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "lr=0.03\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\\\n",
        "                                                       mode = 'min', \\\n",
        "                                                      factor = 0.5,\\\n",
        "                                                      patience = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc-EAnv1HYva",
        "colab_type": "code",
        "outputId": "47f900d7-2960-4f38-ba75-774db9315266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "# training params\n",
        "\n",
        "epochs = 6 \n",
        "\n",
        "counter = 0\n",
        "print_every = 10\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "val_losses = []\n",
        "for e in range(epochs):\n",
        "  \n",
        "    scheduler.step(e)\n",
        "\n",
        "    for seq_tensor, seq_tensor_lengths, label in iter(train_loader):\n",
        "        counter += 1\n",
        "               \n",
        "        seq_tensor = seq_tensor.to(device)\n",
        "        seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "        label = label.to(device)\n",
        " \n",
        "        # get the output from the model\n",
        "        output = net(seq_tensor, seq_tensor_lengths)\n",
        "    \n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output, label.float())\n",
        "        optimizer.zero_grad() \n",
        "        loss.backward()\n",
        "        \n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            \n",
        "            val_losses_in_itr = []\n",
        "            sums = []\n",
        "            sizes = []\n",
        "            \n",
        "            net.eval()\n",
        "            \n",
        "            for seq_tensor, seq_tensor_lengths, label in iter(valid_loader):\n",
        "\n",
        "                seq_tensor = seq_tensor.to(device)\n",
        "                seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "                label = label.to(device)\n",
        "                output = net(seq_tensor, seq_tensor_lengths)\n",
        "                \n",
        "                # losses\n",
        "                val_loss = criterion(output, label.float())     \n",
        "                val_losses_in_itr.append(val_loss.item())\n",
        "                \n",
        "                # accuracy\n",
        "                binary_output = (output >= 0.5).short() # short(): torch.int16\n",
        "                right_or_not = torch.eq(binary_output, label)\n",
        "                sums.append(torch.sum(right_or_not).float().item())\n",
        "                sizes.append(right_or_not.shape[0])\n",
        "                \n",
        "            val_losses.append(np.mean(val_losses_in_itr))\n",
        "            if len(val_losses) > 3:\n",
        "              if val_losses[-2] > val_losses[-1]:\n",
        "                print(\"Save Model...\")\n",
        "                save_model(net, './', 'lstm_model_saved_at_{}.pth'.format(counter))\n",
        "            \n",
        "            accuracy = sum(sums) / sum(sizes)\n",
        "            \n",
        "            net.train()\n",
        "            print(\"Epoch: {:2d}/{:2d}\\t\".format(e+1, epochs),\n",
        "                  \"Steps: {:3d}\\t\".format(counter),\n",
        "                  \"Loss: {:.6f}\\t\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\\t\".format(np.mean(val_losses_in_itr)),\n",
        "                  \"Accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1/ 6\t Steps:  10\t Loss: 0.477550\t Val Loss: 0.652788\t Accuracy: 0.776\n",
            "Epoch:  1/ 6\t Steps:  20\t Loss: 0.442478\t Val Loss: 0.439474\t Accuracy: 0.837\n",
            "Epoch:  1/ 6\t Steps:  30\t Loss: 0.304971\t Val Loss: 0.391255\t Accuracy: 0.876\n",
            "Save Model...\n",
            "Epoch:  1/ 6\t Steps:  40\t Loss: 0.387431\t Val Loss: 0.297274\t Accuracy: 0.912\n",
            "Epoch:  1/ 6\t Steps:  50\t Loss: 0.300540\t Val Loss: 0.849961\t Accuracy: 0.735\n",
            "Save Model...\n",
            "Epoch:  1/ 6\t Steps:  60\t Loss: 0.352602\t Val Loss: 0.301840\t Accuracy: 0.912\n",
            "Save Model...\n",
            "Epoch:  1/ 6\t Steps:  70\t Loss: 0.279997\t Val Loss: 0.220553\t Accuracy: 0.936\n",
            "Epoch:  1/ 6\t Steps:  80\t Loss: 0.257140\t Val Loss: 0.279984\t Accuracy: 0.916\n",
            "Save Model...\n",
            "Epoch:  1/ 6\t Steps:  90\t Loss: 0.195319\t Val Loss: 0.214466\t Accuracy: 0.936\n",
            "Epoch:  1/ 6\t Steps: 100\t Loss: 0.532466\t Val Loss: 0.406762\t Accuracy: 0.835\n",
            "Save Model...\n",
            "Epoch:  1/ 6\t Steps: 110\t Loss: 0.374303\t Val Loss: 0.228697\t Accuracy: 0.950\n",
            "Save Model...\n",
            "Epoch:  2/ 6\t Steps: 120\t Loss: 0.211804\t Val Loss: 0.164227\t Accuracy: 0.956\n",
            "Epoch:  2/ 6\t Steps: 130\t Loss: 0.252083\t Val Loss: 0.172368\t Accuracy: 0.949\n",
            "Save Model...\n",
            "Epoch:  2/ 6\t Steps: 140\t Loss: 0.217899\t Val Loss: 0.159281\t Accuracy: 0.954\n",
            "Epoch:  2/ 6\t Steps: 150\t Loss: 0.204602\t Val Loss: 0.181483\t Accuracy: 0.950\n",
            "Epoch:  2/ 6\t Steps: 160\t Loss: 0.152582\t Val Loss: 0.194513\t Accuracy: 0.944\n",
            "Save Model...\n",
            "Epoch:  2/ 6\t Steps: 170\t Loss: 0.100955\t Val Loss: 0.169698\t Accuracy: 0.951\n",
            "Epoch:  2/ 6\t Steps: 180\t Loss: 0.152443\t Val Loss: 0.198863\t Accuracy: 0.933\n",
            "Save Model...\n",
            "Epoch:  2/ 6\t Steps: 190\t Loss: 0.153467\t Val Loss: 0.173377\t Accuracy: 0.934\n",
            "Save Model...\n",
            "Epoch:  2/ 6\t Steps: 200\t Loss: 0.111577\t Val Loss: 0.157236\t Accuracy: 0.959\n",
            "Save Model...\n",
            "Epoch:  2/ 6\t Steps: 210\t Loss: 0.131446\t Val Loss: 0.153398\t Accuracy: 0.952\n",
            "Save Model...\n",
            "Epoch:  2/ 6\t Steps: 220\t Loss: 0.150310\t Val Loss: 0.152178\t Accuracy: 0.959\n",
            "Save Model...\n",
            "Epoch:  2/ 6\t Steps: 230\t Loss: 0.105885\t Val Loss: 0.120951\t Accuracy: 0.968\n",
            "Save Model...\n",
            "Epoch:  3/ 6\t Steps: 240\t Loss: 0.117156\t Val Loss: 0.113723\t Accuracy: 0.971\n",
            "Epoch:  3/ 6\t Steps: 250\t Loss: 0.027366\t Val Loss: 0.115622\t Accuracy: 0.969\n",
            "Save Model...\n",
            "Epoch:  3/ 6\t Steps: 260\t Loss: 0.069754\t Val Loss: 0.110747\t Accuracy: 0.970\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-4ded0a875b00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AueDDrycs4qa",
        "colab_type": "text"
      },
      "source": [
        "It seemed starting over-fitting at step 250\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpH7ZS3VtxHl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "44f58ec5-ea14-4749-d111-f1ca2c721906"
      },
      "source": [
        "vg = Vocab_to_int()\n",
        "vocab_to_int = vg.open_file()\n",
        "vr = Vectorizer(vocab_to_int)\n",
        "\n",
        "vocab_size = len(vocab_to_int)\n",
        "output_size = 1\n",
        "embedding_dim = 100 # int(vocab_size ** 0.25) # 15\n",
        "hidden_dim = 15\n",
        "n_layers = 2\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
        "model = SpamHamLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, \\\n",
        "                 0.2, 0.2)\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "\n",
        "model_path = './lstm_model_saved_at_240.pth'\n",
        "checkpoint = torch.load(model_path)    \n",
        "state_dict = checkpoint['net']   \n",
        "model.load_state_dict(state_dict) \n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SpamHamLSTM(\n",
            "  (embedding): Embedding(159199, 100)\n",
            "  (lstm): LSTM(100, 15, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.2)\n",
            "  (fc): Linear(in_features=15, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n",
            "embedding.weight torch.Size([159199, 100])\n",
            "lstm.weight_ih_l0 torch.Size([60, 100])\n",
            "lstm.weight_hh_l0 torch.Size([60, 15])\n",
            "lstm.bias_ih_l0 torch.Size([60])\n",
            "lstm.bias_hh_l0 torch.Size([60])\n",
            "lstm.weight_ih_l1 torch.Size([60, 15])\n",
            "lstm.weight_hh_l1 torch.Size([60, 15])\n",
            "lstm.bias_ih_l1 torch.Size([60])\n",
            "lstm.bias_hh_l1 torch.Size([60])\n",
            "fc.weight torch.Size([1, 15])\n",
            "fc.bias torch.Size([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JG5QJ6ZumKw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a75f7589-f692-4604-bd18-4c1924482449"
      },
      "source": [
        "test_losses = []\n",
        "sums = []\n",
        "sizes = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "test_losses = []\n",
        "for seq_tensor, seq_tensor_lengths, label in iter(test_loader):\n",
        "\n",
        "    seq_tensor = seq_tensor.to(device)\n",
        "    seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "    label = label.to(device)\n",
        "    output = model(seq_tensor, seq_tensor_lengths)\n",
        "\n",
        "    # losses\n",
        "    test_loss = criterion(output, label.float())     \n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    # accuracy\n",
        "    binary_output = (output >= 0.5).short() # short(): torch.int16\n",
        "    right_or_not = torch.eq(binary_output, label)\n",
        "    sums.append(torch.sum(right_or_not).float().item())\n",
        "    sizes.append(right_or_not.shape[0])\n",
        "\n",
        "accuracy = np.sum(sums) / np.sum(sizes)\n",
        "print(\"Test Loss: {:.6f}\\t\".format(np.mean(test_losses)),\n",
        "      \"Accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.102844\t Accuracy: 0.971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddkRmBV3vcaz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "9c1d5374-3c06-4fc0-ed58-bb95743ce7ba"
      },
      "source": [
        "sum = 0\n",
        "count = 0\n",
        "model.eval()\n",
        "\n",
        "for idx in range(16500,16600):\n",
        "  count = count + 1\n",
        "  sample = data[idx]\n",
        "  sample_label = label[idx].item()\n",
        "  temp = [ sample ]\n",
        "  seq_tensor, seq_tensor_lengths = vr.vectorize(temp)\n",
        "  \n",
        "  seq_tensor = seq_tensor.to(device)\n",
        "  seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "  output = model(seq_tensor, seq_tensor_lengths)\n",
        "\n",
        "  # accuracy\n",
        "  binary_output = int(output.item() >= 0.5)\n",
        "  acc = int(binary_output == sample_label)\n",
        "  sum = sum + acc\n",
        "  \n",
        "  pred = \"SPAM\" if binary_output == 1 else \"HAM\"\n",
        "  actual = \"SPAM\" if sample_label == 1 else \"HAM\"\n",
        "\n",
        "  if count % 30 == 0:\n",
        "    print(\"Sample\", idx)\n",
        "    print(\"Prediction:\", pred)\n",
        "    print(\"Actual\", actual)\n",
        "    print(\"-------------------------------------\")\n",
        "  \n",
        "print(sum)\n",
        "print(\"Accuracy(%)\", float(sum) / count * 100)\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample 16529\n",
            "Prediction: HAM\n",
            "Actual HAM\n",
            "-------------------------------------\n",
            "Sample 16559\n",
            "Prediction: SPAM\n",
            "Actual SPAM\n",
            "-------------------------------------\n",
            "Sample 16589\n",
            "Prediction: SPAM\n",
            "Actual SPAM\n",
            "-------------------------------------\n",
            "98\n",
            "Accuracy(%) 98.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSxlzF2QRUS9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b53955f8-6903-43a4-e27b-eea06dcf8f7f"
      },
      "source": [
        "myString = \"This is my best offer. You can't take this chance away!\"\n",
        "\n",
        "\n",
        "unnecessary =  [\"-\", \".\", \",\", \"/\", \":\", \"@\", \"'\", \"!\"]\n",
        "content = myString.lower()\n",
        "content = ''.join([c for c in content if c not in unnecessary])\n",
        "\n",
        "\n",
        "input = [content]\n",
        "seq_tensor, seq_tensor_lengths = vr.vectorize(input)\n",
        "\n",
        "seq_tensor = seq_tensor.to(device)\n",
        "seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "output = model(seq_tensor, seq_tensor_lengths)\n",
        "\n",
        "print(output.item()) # 0.98 means strongly SPAM\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9833419322967529\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}