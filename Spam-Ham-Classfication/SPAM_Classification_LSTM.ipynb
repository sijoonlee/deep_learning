{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SPAM_Classification_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uW8XHZIlHxG2",
        "colab": {}
      },
      "source": [
        "#!tar -xopf eron.tar\n",
        "!tar -zxvf data.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9J3UlzLCU9Hj",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "class FileReader(object):\n",
        "  def __init__(self):\n",
        "    self.ham = []\n",
        "    self.spam = []\n",
        "    self.ham_paths = [\"enron1/ham/*.txt\", \"enron2/ham/*.txt\", \"enron3/ham/*.txt\", \"enron4/ham/*.txt\", \"enron5/ham/*.txt\", \"enron6/ham/*.txt\"]\n",
        "    self.spam_paths = [\"enron1/spam/*.txt\", \"enron2/spam/*.txt\", \"enron3/spam/*.txt\", \"enron4/spam/*.txt\", \"enron5/spam/*.txt\", \"enron6/spam/*.txt\"]\n",
        "  \n",
        "  def read_file(self, path, minimum_word_count = 3, unnecessary =  [\"-\", \".\", \",\", \"/\", \":\", \"@\", \"'\", \"!\"]):\n",
        "    files  = glob.glob(path)\n",
        "    content_list = []\n",
        "    for file in files:\n",
        "        with open(file, encoding=\"ISO-8859-1\") as f:\n",
        "            content = f.read()\n",
        "            if len(content.split()) > minimum_word_count:      \n",
        "              content = content.lower()\n",
        "              if len(unnecessary) is not 0:\n",
        "                  content = ''.join([c for c in content if c not in unnecessary])\n",
        "              content_list.append(content)\n",
        "    return content_list\n",
        "  \n",
        "  def truncate_before_combine(self, data, maximum_length = 5000):\n",
        "    if maximum_length is not 0:\n",
        "      if len(data) > maximum_length:\n",
        "        random.shuffle(data)\n",
        "        data = data[:maximum_length]\n",
        "    return data\n",
        "  \n",
        "  def load_ham_and_spam(self, ham_paths = \"default\", spam_paths = \"default\", truncation_length = 5000): # 0 for no truncation\n",
        "    \n",
        "    if ham_paths == \"default\":\n",
        "      ham_paths = self.ham_paths\n",
        "    if spam_paths == \"default\":\n",
        "      spam_paths = self.spam_paths\n",
        "    \n",
        "    self.ham = [ item for path in ham_paths for item in self.read_file(path) ]\n",
        "    if truncation_length != 0:\n",
        "      self.ham = self.truncate_before_combine(self.ham, truncation_length)\n",
        "    print(\"ham length \", len(self.ham))\n",
        "    \n",
        "    self.spam = [item for path in spam_paths for item in self.read_file(path) ]\n",
        "    if truncation_length != 0:\n",
        "      self.spam = self.truncate_before_combine(self.spam, truncation_length)\n",
        "    print(\"spam length \", len(self.spam))\n",
        "    \n",
        "    data = self.ham + self.spam\n",
        "    \n",
        "    ham_label = [0 for _ in range(len(self.ham))]\n",
        "    spam_label = [1 for _ in range(len(self.spam))]\n",
        "    \n",
        "    label_tensor = torch.as_tensor(ham_label + spam_label, dtype = torch.int16)\n",
        "    \n",
        "    return data, label_tensor\n",
        "  \n",
        "  def print_sample(self, which =\"both\"): # ham, spam or both\n",
        "    if which == \"ham\" or which == \"both\":\n",
        "      idx = random.randint(0, len(self.ham))\n",
        "      print(\"----------- ham sample -------------\")\n",
        "      print(self.ham[idx])\n",
        "    if which == \"spam\" or which == \"both\":\n",
        "      idx = random.randint(0, len(self.spam))\n",
        "      print(\"----------- spam sample -------------\")\n",
        "      print(self.spam[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5j3m0dUddcgP",
        "outputId": "52097101-134b-49b3-8950-df8a9e5fdeb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "reader = FileReader()\n",
        "\n",
        "data, label = reader.load_ham_and_spam(\"default\", \"default\", truncation_length = 0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ham length  16540\n",
            "spam length  17108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wg5nJxOMxVQW",
        "outputId": "83cac8f8-5e95-4a78-ed03-565f7af9e2b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "reader.print_sample()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------- ham sample -------------\n",
            "subject re  nymex volumes for rebuttal\n",
            "vince  i need these numbers by tomorrow am as we are at crunch time  thanks\n",
            " chris\n",
            "margaret carson\n",
            "09  27  2000 08  44 am\n",
            "to  chris long  corp  enron  enron\n",
            "cc \n",
            "subject  nymex volumes for rebuttal\n",
            "chris i track physical volumes in markets not really financials    try\n",
            "vince kaminski vp in ena  s\n",
            "research desk and his people will have this for you     margaret\n",
            "----------- spam sample -------------\n",
            "subject quick delivery prescripiton medicine\n",
            "mayer carbonyl allocable cogitate coffer belove\n",
            "find your medications without delay \n",
            "everything you need  we have it  quick and economical \n",
            "you name it  we have them all \n",
            "stop receiving promotional material now\n",
            "truancy deadwood businessman bought\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d3JqHhaQcWmY",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "class Vocab_to_int(object):\n",
        "\n",
        "  def __init__(self, saved_dir='./', file_name=\"vocab_to_int.csv\"):\n",
        "      os.makedirs(saved_dir, exist_ok=True)\n",
        "      self.path = os.path.join(saved_dir, file_name)\n",
        "\n",
        "  def save_file(self, vocab_to_int):\n",
        "      df = pd.DataFrame(list(vocab_to_int.items()))\n",
        "      df.dropna(inplace=True)\n",
        "      df = df.T\n",
        "      df.to_csv(self.path, index=False, header=False)\n",
        "      print(\"saved as\", self.path)\n",
        "\n",
        "  def open_file(self, path = \"Default\"):\n",
        "      if path == \"Default\":\n",
        "        path = self.path\n",
        "      df = pd.read_csv(path)\n",
        "      df.dropna(inplace=True)\n",
        "      dict = df.to_dict('records')[0]\n",
        "      return dict\n",
        "\n",
        "  def generate(self, seqs, save_file=True):\n",
        "      vocabs = [vocab for seq in seqs for vocab in seq.split()]\n",
        "      # a = [  word for seq in [\"a d\",\"b d\",\"c d\"] for word in seq.split() ]\n",
        "      # ['a', 'd', 'b', 'd', 'c', 'd']\n",
        "\n",
        "      # Count word frequency\n",
        "      # Counter({'the': 39770, 'to': 32356, 'and': 22835, .....\n",
        "      vocab_count = Counter(vocabs)\n",
        "\n",
        "      vocab_count = vocab_count.most_common(len(vocab_count))\n",
        "\n",
        "      vocab_to_int = {word : index+2 for index, (word, count) in enumerate(vocab_count)}\n",
        "      vocab_to_int.update({'__PADDING__': 0}) # index 0 for padding\n",
        "      vocab_to_int.update({'__UNKNOWN__': 1}) # index 1 for unknown word such as broken character\n",
        "\n",
        "      if save_file:\n",
        "        self.save_file(vocab_to_int)\n",
        "\n",
        "      return vocab_to_int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IcyiNGm3cZHS",
        "outputId": "e1f601bd-fae1-4d93-89e8-55082084e8e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vti = Vocab_to_int()\n",
        "vocab_to_int = vti.generate(data, save_file=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saved as ./vocab_to_int.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Vdi2zuKHYue",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class Vectorizer(object):\n",
        "  ''' \n",
        "  Using vocab_to_int dict, \n",
        "  Change words into integers in string data\n",
        "  '''\n",
        "  \n",
        "  def __init__(self, vocab_to_int):\n",
        "    self.vocab_to_int = vocab_to_int\n",
        "    \n",
        "  def vectorize_seqs(self, seqs):\n",
        "    # Vectorize each sequence\n",
        "    vectorized_seqs = []\n",
        "    for seq in seqs: \n",
        "      vectorized_seqs.append([self.vocab_to_int.get(word, 1) for word in seq.split()])\n",
        "      # self.vocab_to_int.get(word, 1) ; mean if no value for key, it will return 1 (unknown)\n",
        "    return vectorized_seqs\n",
        "  \n",
        "  def add_padding(self, vectorized_seqs, seq_lengths):\n",
        "    '''\n",
        "    The length of the seq_tensor is the length of the longest sentence in the data\n",
        "    The shorter sentences will have padding(zero) at the their ends\n",
        "    '''\n",
        "    seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
        "    for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "      seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "    return seq_tensor\n",
        "  \n",
        "  def vectorize(self, seqs):\n",
        "    vectorized_seqs = self.vectorize_seqs(seqs)\n",
        "    seq_lengths = torch.LongTensor(list(map(len, vectorized_seqs)))\n",
        "    seq_tensor = self.add_padding(vectorized_seqs, seq_lengths)\n",
        "    \n",
        "    return seq_tensor, seq_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pA1eZE5mouSo",
        "colab": {}
      },
      "source": [
        "v = Vectorizer(vocab_to_int)\n",
        "seq_tensor, seq_lengths = v.vectorize(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PT6HTGm3HYu7",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "class DataDivider(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        init with seq_tensor and label, both of them are torch.tensor\n",
        "        '''        \n",
        "    def check_length(self, seq_tensor, seq_lengths, label):\n",
        "        length = len(label)\n",
        "        if len(seq_tensor) != length or len(seq_lengths) != length:\n",
        "            print(\"The lengths doesn't match with each other\")\n",
        "            print(\"seq_tensor:\", len(seq_tensor))\n",
        "            print(\"seq_length:\", len(seq_lengths))\n",
        "            print(\"label:\", length)\n",
        "            return False\n",
        "          \n",
        "        return True\n",
        "    \n",
        "    def shuffle(self, seq_tensor, seq_lengths, label):\n",
        "        shuffled_idx = torch.randperm(label.shape[0])\n",
        "        seq_tensor = seq_tensor[shuffled_idx]\n",
        "        seq_lengths = seq_lengths[shuffled_idx]\n",
        "        label = label[shuffled_idx]\n",
        "        return seq_tensor, seq_lengths, label\n",
        "        \n",
        "    def divide_train_valid_test(self, seq_tensor, seq_lengths, label, PCT_TRAIN = 0.7, PCT_VALID = 0.2, do_shuffle= True):\n",
        "        '''\n",
        "        PCT_TRAIN: the percent of train set\n",
        "        PCT_VALID: the percent of validation set\n",
        "        The rest part will be the test set\n",
        "        '''\n",
        "        assert self.check_length(seq_tensor, seq_lengths, label)\n",
        "        \n",
        "        length = len(label)\n",
        "        \n",
        "        if do_shuffle:\n",
        "            seq_tensor, seq_lengths, label = self.shuffle(seq_tensor, seq_lengths, label)\n",
        "\n",
        "        train_seq_tensor = seq_tensor[:int(length*PCT_TRAIN)] \n",
        "        train_seq_lengths = seq_lengths[:int(length*PCT_TRAIN)]\n",
        "        train_label = label[:int(length*PCT_TRAIN)]\n",
        "\n",
        "        valid_seq_tensor = seq_tensor[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
        "        valid_seq_lengths = seq_lengths[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
        "        valid_label = label[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))]\n",
        "\n",
        "        test_seq_tensor = seq_tensor[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "        test_seq_lengths = seq_lengths[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "        test_label = label[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "\n",
        "        print(\"train:\", train_seq_tensor.shape)\n",
        "        print(\"valid:\", valid_seq_tensor.shape)\n",
        "        print(\"test:\", test_seq_tensor.shape)\n",
        "        \n",
        "        return train_seq_tensor, train_seq_lengths, train_label, \\\n",
        "                valid_seq_tensor, valid_seq_lengths, valid_label, \\\n",
        "                test_seq_tensor, test_seq_lengths, test_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_9ADJtWW0bv0",
        "outputId": "8a25d448-1f2a-45b2-c74a-fff6e30fc27b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "dd = DataDivider()\n",
        "train_seq_tensor, train_seq_lengths, train_label, \\\n",
        "valid_seq_tensor, valid_seq_lengths, valid_label, \\\n",
        "test_seq_tensor, test_seq_lengths, test_label = dd.divide_train_valid_test(seq_tensor, seq_lengths, label)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: torch.Size([23553, 38538])\n",
            "valid: torch.Size([6730, 38538])\n",
            "test: torch.Size([3365, 38538])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QN5DLcuBv-tf",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.utils.data.sampler as splr\n",
        "\n",
        "class CustomDataLoader(object):\n",
        "  def __init__(self, seq_tensor, seq_lengths, label_tensor, batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_tensor = seq_tensor\n",
        "    self.seq_lengths = seq_lengths\n",
        "    self.label_tensor = label_tensor\n",
        "    self.sampler = splr.BatchSampler(splr.RandomSampler(self.label_tensor), self.batch_size, False)\n",
        "    self.sampler_iter = iter(self.sampler)\n",
        "    \n",
        "  def __iter__(self):\n",
        "    self.sampler_iter = iter(self.sampler) # reset sampler iterator\n",
        "    return self\n",
        "\n",
        "  def _next_index(self):\n",
        "    return next(self.sampler_iter) # may raise StopIteration\n",
        "\n",
        "  def __next__(self):\n",
        "    index = self._next_index()\n",
        "\n",
        "    subset_seq_tensor = self.seq_tensor[index]\n",
        "    subset_seq_lengths = self.seq_lengths[index]\n",
        "    subset_label_tensor = self.label_tensor[index]\n",
        "\n",
        "    subset_seq_lengths, perm_idx = subset_seq_lengths.sort(0, descending=True)\n",
        "    subset_seq_tensor = subset_seq_tensor[perm_idx]\n",
        "    subset_label_tensor = subset_label_tensor[perm_idx]\n",
        "\n",
        "    return subset_seq_tensor, subset_seq_lengths, subset_label_tensor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sampler)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ol-GoapHYvE",
        "colab": {}
      },
      "source": [
        "batch_size = 200\n",
        "train_loader = CustomDataLoader(train_seq_tensor, train_seq_lengths, train_label, batch_size)\n",
        "valid_loader = CustomDataLoader(valid_seq_tensor, valid_seq_lengths, valid_label, batch_size)\n",
        "test_loader = CustomDataLoader(test_seq_tensor, test_seq_lengths, test_label, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5aYZ1Q3Jw4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del dd\n",
        "del v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NgU1rRGlHYvO",
        "colab": {}
      },
      "source": [
        "# Define Model\n",
        "'''\n",
        "1) Embedding Layer\n",
        "2) LSTM\n",
        "3) Fully Connected Layer\n",
        "4) Sigmoid Activation\n",
        "'''\n",
        "\n",
        "DEBUG = False\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class SpamHamLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers,\\\n",
        "                 drop_out_in_lstm, drop_out, output_size, device):\n",
        "\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_out_in_lstm, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, seq_lengths):\n",
        "\n",
        "        # embeddings\n",
        "        embedded_seq_tensor = self.embedding(x)\n",
        "        if DEBUG:\n",
        "          print(\"embedded_seq_tensor = self.embedding(x)\", embedded_seq_tensor.shape)\n",
        "                \n",
        "        # pack, remove pads\n",
        "        packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\n",
        "        if DEBUG:\n",
        "          print(\"packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\")\n",
        "          print(packed_input.data.shape)\n",
        "          print(packed_input.batch_sizes.shape)\n",
        "        \n",
        "        # lstm\n",
        "        packed_output, (ht, ct) = self.lstm(packed_input, None)\n",
        "        if DEBUG:\n",
        "          print(\"packed_output, (ht, ct) = self.lstm(packed_input, None)\")\n",
        "          print(packed_output.data.shape)\n",
        "          print(packed_output.batch_sizes.shape)\n",
        "          print(\"ht\")\n",
        "          print(ht.shape)\n",
        "        \n",
        "        # unpack, recover padded sequence\n",
        "        output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
        "        # output : batch_size X max_seq_len X hidden_dim\n",
        "        if DEBUG:\n",
        "          print(\"output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\")\n",
        "          print(output.shape)\n",
        "          print(input_sizes)\n",
        "       \n",
        "        # gather the last output in each batch\n",
        "        last_idxs = (input_sizes - 1).to(self.device) # last_idxs = input_sizes - torch.ones_like(input_sizes)\n",
        "        output = torch.gather(output, 1, last_idxs.view(-1, 1).unsqueeze(2).repeat(1, 1, self.hidden_dim)).squeeze() # [batch_size, hidden_dim]\n",
        "        if DEBUG:\n",
        "          print(output.shape) \n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc(output).squeeze()\n",
        "        if DEBUG:\n",
        "          print(\"output = self.fc(output)\", output.shape)\n",
        "               \n",
        "        # sigmoid function\n",
        "        output = self.sig(output)\n",
        "        \n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o3doWR825pMo",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Model_wrapper(object):\n",
        "  \n",
        "\tdef set_params(self, vocab_size, \\\n",
        "\t\t\t\t\t   embedding_dim = 100, \\\n",
        "\t\t\t\t\t   hidden_dim = 15, \\\n",
        "\t\t\t\t\t   n_layers = 2, \\\n",
        "\t\t\t\t\t   drop_out_in_lstm = 0.2, \\\n",
        "\t\t\t\t\t   drop_out = 0.2, \\\n",
        "\t\t\t\t\t   output_size = 1, \\\n",
        "\t\t\t\t\t   train_on_gpu = True):\n",
        "    \n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_dim = embedding_dim\n",
        "\t\tself.hidden_dim = hidden_dim\n",
        "\t\tself.n_layers = 2\n",
        "\t\tself.drop_out_in_lstm = drop_out_in_lstm\n",
        "\t\tself.drop_out = drop_out\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.train_on_gpu = train_on_gpu\n",
        "\t\tself.device = \"cuda\" if torch.cuda.is_available() and train_on_gpu else \"cpu\" \n",
        "\n",
        "\tdef set_model(self, do_print = True):\n",
        "\t\tself.model = SpamHamLSTM(self.vocab_size, self.embedding_dim, self.hidden_dim, self.n_layers, \\\n",
        "\t\t\t\t\t self.drop_out_in_lstm, self.drop_out, self.output_size, self.device)\n",
        "\t\tself.model = self.model.to(self.device)\n",
        "\t\tif do_print:\n",
        "\t\t\tprint(self.model)\n",
        "\n",
        "\tdef train(self, train_loader, valid_loader, criterion = \"default\", optimizer=\"default\", learning_rate = 0.03, use_scheduler = True, \\\n",
        "         epochs = 6, validate_every = 10, gradient_clip = 5):\n",
        "\n",
        "\t\tif criterion == \"default\" :\n",
        "\t\t\tcriterion = nn.BCELoss()\n",
        "\t\tprint(criterion)\n",
        "     \n",
        "\n",
        "\t\tif optimizer == \"default\" :\n",
        "\t\t\toptimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "\t\tprint(optimizer)\n",
        "\n",
        "\t\tif use_scheduler :\n",
        "\t\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.5, patience = 2)\n",
        "\n",
        "\t\tcounter = 0\n",
        "\n",
        "\t\tself.model.train()\n",
        "\t\n",
        "\t\tval_losses = []\n",
        "\t\tval_min_loss = 1000000\n",
        "\n",
        "\t\tfor e in range(epochs):\n",
        "\n",
        "\t\t\tif use_scheduler :\n",
        "\t\t\t\tscheduler.step(e)\n",
        "\n",
        "\t\t\tfor seq_tensor, seq_tensor_lengths, label in iter(train_loader):\n",
        "\t\t\t\tcounter += 1\n",
        "\n",
        "\t\t\t\tseq_tensor = seq_tensor.to(self.device)\n",
        "\t\t\t\tseq_tensor_lengths = seq_tensor_lengths.to(self.device)\n",
        "\t\t\t\tlabel = label.to(self.device)\n",
        "\n",
        "\t\t\t\t# get the output from the model\n",
        "\t\t\t\toutput = self.model(seq_tensor, seq_tensor_lengths)\n",
        "\n",
        "\t\t\t\t# calculate the loss and perform backprop\n",
        "\t\t\t\tloss = criterion(output, label.float())\n",
        "\t\t\t\toptimizer.zero_grad() \n",
        "\t\t\t\tloss.backward()\n",
        "\n",
        "\t\t\t\t# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "\t\t\t\tnn.utils.clip_grad_norm_(self.model.parameters(), gradient_clip)\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\t# loss stats\n",
        "\t\t\t\tif counter % validate_every == 0:\n",
        "\t\t\t\t\t# Get validation loss\n",
        "\t\t\t\t\tval_losses_in_itr = []\n",
        "\t\t\t\t\tsums = []\n",
        "\t\t\t\t\tsizes = []\n",
        "\n",
        "\t\t\t\t\tself.model.eval()\n",
        "\n",
        "\t\t\t\t\tfor seq_tensor, seq_tensor_lengths, label in iter(valid_loader):\n",
        "\n",
        "\t\t\t\t\t\tseq_tensor = seq_tensor.to(self.device)\n",
        "\t\t\t\t\t\tseq_tensor_lengths = seq_tensor_lengths.to(self.device)\n",
        "\t\t\t\t\t\tlabel = label.to(self.device)\n",
        "\t\t\t\t\t\toutput = self.model(seq_tensor, seq_tensor_lengths)\n",
        "\n",
        "\t\t\t\t\t\t# losses\n",
        "\t\t\t\t\t\tval_loss = criterion(output, label.float())     \n",
        "\t\t\t\t\t\tval_losses_in_itr.append(val_loss.item())\n",
        "\n",
        "\t\t\t\t\t\t# accuracy\n",
        "\t\t\t\t\t\tbinary_output = (output >= 0.5).short() # short(): torch.int16\n",
        "\t\t\t\t\t\tright_or_not = torch.eq(binary_output, label)\n",
        "\t\t\t\t\t\tsums.append(torch.sum(right_or_not).float().item())\n",
        "\t\t\t\t\t\tsizes.append(right_or_not.shape[0])\n",
        "\n",
        "\t\t\t\t\tval_losses.append(np.mean(val_losses_in_itr))\n",
        "\t\t\t\t\tif val_min_loss > val_losses[-1]:\n",
        "\t\t\t\t\t\tval_min_loss = val_losses[-1]\n",
        "\t\t\t\t\t\tself.save_state_dict('./', 'lstm_model_saved_at_{}.pth'.format(counter))\n",
        "\n",
        "\t\t\t\t\taccuracy = np.sum(sums) / np.sum(sizes)\n",
        "\n",
        "\t\t\t\t\tself.model.train()\n",
        "\t\t\t\t\tprint(\"Epoch: {:2d}/{:2d}\\t\".format(e+1, epochs),\n",
        "\t\t\t\t\t\t  \"Steps: {:3d}\\t\".format(counter),\n",
        "\t\t\t\t\t\t  \"Loss: {:.5f}\\t\".format(loss.item()),\n",
        "\t\t\t\t\t\t  \"Val Loss: {:.5f}\\t\".format(np.mean(val_losses_in_itr)),\n",
        "\t\t\t\t\t\t  \"Accuracy: {:.3f}\".format(accuracy))    \n",
        "\n",
        "\tdef test(self, test_loader, criterion = \"default\"):\n",
        "\t\tif criterion == \"default\":\n",
        "\t\t\tcriterion = nn.BCELoss()\n",
        "\n",
        "\t\ttest_losses = []\n",
        "\t\tsums = []\n",
        "\t\tsizes = []\n",
        "\n",
        "\t\tself.model.eval()\n",
        "\n",
        "\t\ttest_losses = []\n",
        "\t\t\n",
        "\t\tfor seq_tensor, seq_tensor_lengths, label in iter(test_loader):\n",
        "\t\t\tseq_tensor = seq_tensor.to(self.device)\n",
        "\t\t\tseq_tensor_lengths = seq_tensor_lengths.to(self.device)\n",
        "\t\t\tlabel = label.to(self.device)\n",
        "\t\t\toutput = self.model(seq_tensor, seq_tensor_lengths)\n",
        "\n",
        "\t\t\t# losses\n",
        "\t\t\ttest_loss = criterion(output, label.float())     \n",
        "\t\t\ttest_losses.append(test_loss.item())\n",
        "\n",
        "\t\t\t# accuracy\n",
        "\t\t\tbinary_output = (output >= 0.5).short() # short(): torch.int16\n",
        "\t\t\tright_or_not = torch.eq(binary_output, label)\n",
        "\t\t\tsums.append(torch.sum(right_or_not).float().item())\n",
        "\t\t\tsizes.append(right_or_not.shape[0])\n",
        "\n",
        "\t\taccuracy = np.sum(sums) / np.sum(sizes)\n",
        "\t\tprint(\"Test Loss: {:.6f}\\t\".format(np.mean(test_losses)),\n",
        "\t\t\"Accuracy: {:.3f}\".format(accuracy))\n",
        "    \n",
        "\tdef load_state_dict(self, saved_dir='./', file_name='saved_model.pth', do_print = True):\n",
        "\t\toutput_path = os.path.join(saved_dir, file_name)\n",
        "\t\tcheckpoint = torch.load(output_path, map_location=self.device)\n",
        "\t\tstate_dict = checkpoint['net']\n",
        "\t\tself.model.load_state_dict(state_dict)\n",
        "\t\tif do_print:\n",
        "\t\t\tfor name, param in self.model.named_parameters():\n",
        "\t\t\t\tif param.requires_grad:\n",
        "\t\t\t\t\tprint(name, param.data.shape)\n",
        "\n",
        "\tdef save_state_dict(self, saved_dir='./', file_name='saved_model.pth', do_print = True):\n",
        "\t\tos.makedirs(saved_dir, exist_ok=True)\n",
        "\t\tcheck_point = {\n",
        "\t\t\t'net': self.model.state_dict()\n",
        "\t\t}\n",
        "\t\toutput_path = os.path.join(saved_dir, file_name)\n",
        "\t\ttorch.save(check_point, output_path)\n",
        "\t\tif do_print:\n",
        "\t\t\tprint(\"saved as\", output_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FFjoOB0mfq9v",
        "outputId": "2d17b517-8508-4959-d6ba-e69dbe2adbd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "vocab_size = len(vocab_to_int)\n",
        "print(vocab_size)\n",
        "mw = Model_wrapper()\n",
        "mw.set_params(vocab_size)\n",
        "mw.set_model()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "159198\n",
            "SpamHamLSTM(\n",
            "  (embedding): Embedding(159198, 100)\n",
            "  (lstm): LSTM(100, 15, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.2)\n",
            "  (fc): Linear(in_features=15, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BugCrW2bytwI",
        "outputId": "357e3453-8454-43de-b3a3-28b15c1fc789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mw.train(train_loader, valid_loader)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BCELoss()\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.03\n",
            "    weight_decay: 0\n",
            ")\n",
            "saved as ./lstm_model_saved_at_10.pth\n",
            "Epoch:  1/ 6\t Steps:  10\t Loss: 0.52850\t Val Loss: 0.40924\t Accuracy: 0.836\n",
            "saved as ./lstm_model_saved_at_20.pth\n",
            "Epoch:  1/ 6\t Steps:  20\t Loss: 0.32492\t Val Loss: 0.24394\t Accuracy: 0.922\n",
            "saved as ./lstm_model_saved_at_30.pth\n",
            "Epoch:  1/ 6\t Steps:  30\t Loss: 0.27150\t Val Loss: 0.22049\t Accuracy: 0.926\n",
            "Epoch:  1/ 6\t Steps:  40\t Loss: 0.42186\t Val Loss: 0.29834\t Accuracy: 0.881\n",
            "Epoch:  1/ 6\t Steps:  50\t Loss: 0.23898\t Val Loss: 0.22442\t Accuracy: 0.911\n",
            "saved as ./lstm_model_saved_at_60.pth\n",
            "Epoch:  1/ 6\t Steps:  60\t Loss: 0.24825\t Val Loss: 0.14612\t Accuracy: 0.951\n",
            "saved as ./lstm_model_saved_at_70.pth\n",
            "Epoch:  1/ 6\t Steps:  70\t Loss: 0.09223\t Val Loss: 0.09881\t Accuracy: 0.965\n",
            "saved as ./lstm_model_saved_at_80.pth\n",
            "Epoch:  1/ 6\t Steps:  80\t Loss: 0.11209\t Val Loss: 0.08500\t Accuracy: 0.971\n",
            "saved as ./lstm_model_saved_at_90.pth\n",
            "Epoch:  1/ 6\t Steps:  90\t Loss: 0.10654\t Val Loss: 0.07252\t Accuracy: 0.976\n",
            "saved as ./lstm_model_saved_at_100.pth\n",
            "Epoch:  1/ 6\t Steps: 100\t Loss: 0.09192\t Val Loss: 0.06148\t Accuracy: 0.981\n",
            "Epoch:  1/ 6\t Steps: 110\t Loss: 0.06131\t Val Loss: 0.06384\t Accuracy: 0.980\n",
            "saved as ./lstm_model_saved_at_120.pth\n",
            "Epoch:  2/ 6\t Steps: 120\t Loss: 0.04945\t Val Loss: 0.05517\t Accuracy: 0.983\n",
            "saved as ./lstm_model_saved_at_130.pth\n",
            "Epoch:  2/ 6\t Steps: 130\t Loss: 0.03868\t Val Loss: 0.04536\t Accuracy: 0.986\n",
            "saved as ./lstm_model_saved_at_140.pth\n",
            "Epoch:  2/ 6\t Steps: 140\t Loss: 0.01129\t Val Loss: 0.04530\t Accuracy: 0.985\n",
            "Epoch:  2/ 6\t Steps: 150\t Loss: 0.06652\t Val Loss: 0.06506\t Accuracy: 0.977\n",
            "Epoch:  2/ 6\t Steps: 160\t Loss: 0.01907\t Val Loss: 0.04634\t Accuracy: 0.984\n",
            "Epoch:  2/ 6\t Steps: 170\t Loss: 0.03247\t Val Loss: 0.05971\t Accuracy: 0.981\n",
            "saved as ./lstm_model_saved_at_180.pth\n",
            "Epoch:  2/ 6\t Steps: 180\t Loss: 0.03144\t Val Loss: 0.04486\t Accuracy: 0.985\n",
            "Epoch:  2/ 6\t Steps: 190\t Loss: 0.01312\t Val Loss: 0.04497\t Accuracy: 0.986\n",
            "saved as ./lstm_model_saved_at_200.pth\n",
            "Epoch:  2/ 6\t Steps: 200\t Loss: 0.02971\t Val Loss: 0.04364\t Accuracy: 0.986\n",
            "Epoch:  2/ 6\t Steps: 210\t Loss: 0.06865\t Val Loss: 0.04546\t Accuracy: 0.985\n",
            "saved as ./lstm_model_saved_at_220.pth\n",
            "Epoch:  2/ 6\t Steps: 220\t Loss: 0.01515\t Val Loss: 0.04308\t Accuracy: 0.985\n",
            "saved as ./lstm_model_saved_at_230.pth\n",
            "Epoch:  2/ 6\t Steps: 230\t Loss: 0.01172\t Val Loss: 0.04206\t Accuracy: 0.988\n",
            "Epoch:  3/ 6\t Steps: 240\t Loss: 0.01545\t Val Loss: 0.06375\t Accuracy: 0.982\n",
            "Epoch:  3/ 6\t Steps: 250\t Loss: 0.00339\t Val Loss: 0.05243\t Accuracy: 0.985\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-716472d52336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-8c5cfa716bd2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, valid_loader, criterion, optimizer, learning_rate, use_scheduler, epochs, validate_every, gradient_clip)\u001b[0m\n\u001b[1;32m     82\u001b[0m                                                 \u001b[0mseq_tensor_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_tensor_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                                                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                                                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_tensor_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                                 \u001b[0;31m# losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-9d44194a2a10>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, seq_lengths)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# lstm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mpacked_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"packed_output, (ht, ct) = self.lstm(packed_input, None)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_packed\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mmax_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n\u001b[0;32m--> 525\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X1l8r0hKDDUL"
      },
      "source": [
        "Interrupt the training, model at step 380 will be used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ri_4O9ANEtvY",
        "outputId": "06436bcb-47dd-4214-8577-96baa198b574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mw.load_state_dict(saved_dir='./', file_name='lstm_model_saved_at_230.pth', do_print = False)\n",
        "mw.test(test_loader)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.044377\t Accuracy: 0.987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Oju9wsvLf_j_",
        "outputId": "dfb52e49-41e7-4ff4-a56a-2d9f11d626fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mw.load_state_dict(saved_dir='./', file_name='lstm_model_saved_at_130.pth', do_print = False)\n",
        "mw.test(test_loader)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.049122\t Accuracy: 0.985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h8SudydcFsya",
        "outputId": "dad4a0aa-e50f-41da-b7fa-271056fc690b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mw.load_state_dict(saved_dir='./', file_name='lstm_model_saved_at_20.pth', do_print = False)\n",
        "mw.test(test_loader)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.245777\t Accuracy: 0.919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xEHYXkKAGgJi",
        "colab": {}
      },
      "source": [
        "class Predictor(Model_wrapper):\n",
        "\tdef __init__(self, saved_dir='./', file_name = 'lstm_model_saved_at_380.pth'):\n",
        "\t\tvg = Vocab_to_int()\n",
        "\t\tself.vocab_to_int = vg.open_file()\n",
        "\t\tvocab_size = len(self.vocab_to_int)\n",
        "\t\tself.vr = Vectorizer(self.vocab_to_int)\n",
        "\t\tself.set_params(vocab_size, train_on_gpu = False)\n",
        "\t\tself.set_model(do_print = False)\n",
        "\t\tself.load_state_dict(saved_dir, file_name, do_print = False)\n",
        "\n",
        "\tdef predict(self, text, unnecessary = [\"-\", \".\", \",\", \"/\", \":\", \"@\", \"'\", \"!\"]):\t\n",
        "\t\ttext = text.lower()\n",
        "\t\ttext = ''.join([c for c in text if c not in unnecessary])\n",
        "\t\ttext = [text]\n",
        "\t\tseq_tensor, seq_tensor_lengths = self.vr.vectorize(text)\n",
        "\t\tseq_tensor_lengths = seq_tensor_lengths\n",
        "\t\tself.model.eval()\n",
        "\t\toutput = self.model(seq_tensor, seq_tensor_lengths)\n",
        "\t\treturn output.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cSxlzF2QRUS9",
        "outputId": "dd4ee7a8-e81c-4b3a-cff1-9e953fd2f375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "p = Predictor(file_name = 'lstm_model_saved_at_230.pth')\n",
        "\n",
        "myString = \"This is the greatest offer. You can't take this chance away! We offer the best product in the world\"\n",
        "result = p.predict(myString)\n",
        "print(result)\n",
        "\n",
        "myString = \"Hello, we have meeting with boss at 1:00 pm. Please prepare the document. I'll be there earlier, we need to discuss before the meeting\"\n",
        "result = p.predict(myString)\n",
        "print(result)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9955840706825256\n",
            "0.0006301779649220407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Z5vv1VofvMW",
        "outputId": "416c1c59-2857-4f53-e49e-64c7bde159e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "p = Predictor(file_name = 'lstm_model_saved_at_130.pth')\n",
        "\n",
        "myString = \"This is the greatest offer. You can't take this chance away! We offer the best product in the world\"\n",
        "result = p.predict(myString)\n",
        "print(result)\n",
        "\n",
        "myString = \"Hello, we have meeting with boss at 1:00 pm. Please prepare the document. I'll be there earlier, we need to discuss before the meeting\"\n",
        "result = p.predict(myString)\n",
        "print(result)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9969056248664856\n",
            "0.023785647004842758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4fJpXgRzduTX",
        "outputId": "5ec2f643-de7a-40f9-cf52-571c37e1ad7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "p = Predictor(file_name = 'lstm_model_saved_at_20.pth')\n",
        "\n",
        "myString = \"This is the greatest offer. You can't take this chance away! We offer the best product in the world\"\n",
        "result = p.predict(myString)\n",
        "print(result)\n",
        "\n",
        "myString = \"Hello, we have meeting with boss at 1:00 pm. Please prepare the document. I'll be there earlier, we need to discuss before the meeting\"\n",
        "result = p.predict(myString)\n",
        "print(result)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9177760481834412\n",
            "0.025812486186623573\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}