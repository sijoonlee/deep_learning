{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "SPAM Classification LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW8XHZIlHxG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xopf eron.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J3UlzLCU9Hj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "class FileReader(object):\n",
        "\n",
        "  def read_file(self, path, minimum_word_count = 3, unnecessary =  [\"-\", \".\", \",\", \"/\", \":\", \"@\"]):\n",
        "    files  = glob.glob(path)\n",
        "    content_list = []\n",
        "    for file in files:\n",
        "        with open(file, encoding=\"ISO-8859-1\") as f:\n",
        "            content = f.read()\n",
        "            if len(content.split()) > minimum_word_count:      \n",
        "              content = content.lower()\n",
        "              if len(unnecessary) is not 0:\n",
        "                  content = ''.join([c for c in content if c not in unnecessary])\n",
        "              content_list.append(content)\n",
        "    return content_list\n",
        "  \n",
        "  def truncate_data(self, data, maximum_length = 5000):\n",
        "    if maximum_length is not 0:\n",
        "      if len(data) > maximum_length:\n",
        "        random.shuffle(data)\n",
        "        data = data[:maximum_length]\n",
        "    return data\n",
        "  \n",
        "  def run(self, ham_paths = [\"enron1/ham/*.txt\", \"enron2/ham/*.txt\", \"enron3/ham/*.txt\", \"enron4/ham/*.txt\", \"enron5/ham/*.txt\", \"enron6/ham/*.txt\"],\\\n",
        "          spam_paths = [\"enron1/spam/*.txt\", \"enron2/spam/*.txt\", \"enron3/spam/*.txt\", \"enron4/spam/*.txt\", \"enron5/spam/*.txt\", \"enron6/spam/*.txt\"]):\n",
        "    \n",
        "    ham = [ item for path in ham_paths for item in self.read_file(path) ]\n",
        "    ham = self.truncate_data(ham)\n",
        "    print(\"ham length \", len(ham))\n",
        "    \n",
        "    spam = [item for path in spam_paths for item in self.read_file(path) ]\n",
        "    spam = self.truncate_data(spam)\n",
        "    print(\"spam length \", len(spam))\n",
        "    \n",
        "    data = ham + spam\n",
        "    \n",
        "    ham_label = [0 for _ in range(len(ham))]\n",
        "    spam_label = [1 for _ in range(len(spam))]\n",
        "    \n",
        "    label_tensor = torch.as_tensor(ham_label + spam_label, dtype = torch.int16)\n",
        "    \n",
        "    return data, label_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j3m0dUddcgP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "eafcd7c8-09e0-45e5-b82e-c671b4f69195"
      },
      "source": [
        "  \n",
        "reader = FileReader()\n",
        "\n",
        "data, label = reader.run()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ham length  5000\n",
            "spam length  5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg5nJxOMxVQW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "3ca770c2-777e-4139-b511-bcfe25fe12bc"
      },
      "source": [
        "print(data[5002])\n",
        "print(label[4998:5002])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "subject lose your weight  new weightloss loses up to 19 % \n",
            "hello  i have a special offer for you   \n",
            "want to lose weight ?\n",
            "the most powerful weightloss is now available\n",
            "without prescription  all natural adipren 720\n",
            "100 % money back guarant?e !\n",
            " lose up to 19 % total body weight \n",
            " up to 300 % more weight loss while dieting \n",
            " loss of 20  35 % abdominal fat \n",
            " reduction of 40  70 % overall fat under skin \n",
            " increase metabolic rate by 76  9 % without exercise \n",
            " burns calorized fat \n",
            " suppresses appetite for sugar \n",
            " boost your confidence level and self esteem \n",
            "get the facts about all  natural adipren 720  http    adiprenl 2  com \n",
            "    system information    \n",
            "natural represents know and operation via part replaced\n",
            "sender uses mistake resource native sends request specific\n",
            "it development implemented simplified usage own because while\n",
            "scenarios no marks include functionality disclose development sends\n",
            "setting replaced create when forethought variation segments helpful\n",
            "\n",
            "tensor([0, 0, 1, 1], dtype=torch.int16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Vdi2zuKHYue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "arg: ham or spam data (numpy array)\n",
        "return: int dictionary [ word_n: count_n, ... ]\n",
        "'''\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "\n",
        "class Vectorizer(object):\n",
        "  \n",
        "  def __init__(self, seqs):\n",
        "    self.vectorized_seqs, self.vocab_int = self.vectorize_seqs(seqs)\n",
        "    self.seq_lengths = torch.LongTensor(list(map(len, self.vectorized_seqs)))\n",
        "    self.seq_tensor = self.add_padding(self.vectorized_seqs, self.seq_lengths)\n",
        "    \n",
        "  def vectorize_seqs(self, seqs):\n",
        "    # sequence of words\n",
        "    vocabs = [vocab for seq in seqs for vocab in seq.split()]\n",
        "      # a = [  word for seq in [\"a d\",\"b d\",\"c d\"] for word in seq.split() ]\n",
        "      # ['a', 'd', 'b', 'd', 'c', 'd']\n",
        "\n",
        "    # Count word frequency\n",
        "    # Counter({'the': 39770, 'to': 32356, 'and': 22835, 'of': 19607, 'a': 17100, '_': 16955, 'you': 15593, 'in': 14481, .....\n",
        "    vocab_count = Counter(vocabs)\n",
        "\n",
        "    # Sort by Freq\n",
        "    vocab_count = vocab_count.most_common(len(vocab_count))\n",
        "\n",
        "    vocab_int = {word : index+1 for index, (word, count) in enumerate(vocab_count)}\n",
        "    vocab_int.update({'__PADDING__': 0}) # index 0 for padding\n",
        "\n",
        "    # Vectorize each sequence\n",
        "    vectorized_seqs = []\n",
        "    for seq in seqs: \n",
        "      vectorized_seqs.append([vocab_int[word] for word in seq.split()])\n",
        "\n",
        "    return vectorized_seqs, vocab_int\n",
        "  \n",
        "  def add_padding(self, vectorized_seqs, seq_lengths):\n",
        "    seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
        "    for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "      seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "    return seq_tensor\n",
        "  \n",
        "  def result(self):\n",
        "    return self.seq_tensor, self.seq_lengths, self.vocab_int\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "# def padding(vectorized_seqs):\n",
        "#     sorted_seqs = sorted(vectorized_seqs, key=lambda x:len(x))\n",
        "#     size = len(sorted_seqs[-1]) # the longest one will be the size of input to the model\n",
        "#     for i, x in enumerate(vectorized_seqs):\n",
        "#         missing = size - len(x)\n",
        "#         vectorized_seqs[i] = vectorized_seqs[i] + [0 for _ in range(missing)] # 0 is padding\n",
        "        \n",
        "#     return vectorized_seqs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA1eZE5mouSo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "1bb4a23a-69f9-4870-87c2-17b5fb216847"
      },
      "source": [
        "vectorizer = Vectorizer(data)\n",
        "seq_tensor, seq_lengths, vocab_int = vectorizer.result()\n",
        "\n",
        "print(seq_tensor[:5])\n",
        "print(seq_lengths[:5])\n",
        "print(seq_tensor.shape)\n",
        "print(len(vocab_int))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[  24, 4311, 4810,  ...,    0,    0,    0],\n",
            "        [  24,   12, 3066,  ...,    0,    0,    0],\n",
            "        [  24,   46,   46,  ...,    0,    0,    0],\n",
            "        [  24,  607, 3066,  ...,    0,    0,    0],\n",
            "        [  24,   88, 1503,  ...,    0,    0,    0]])\n",
            "tensor([  83,   18,   63,  124, 1131])\n",
            "torch.Size([10000, 39470])\n",
            "87476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN5DLcuBv-tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.utils.data.sampler as splr\n",
        "\n",
        "\n",
        "class CustomDataLoader(object):\n",
        "  def __init__(self, seq_tensor, seq_lengths, label_tensor, batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_tensor = seq_tensor\n",
        "    self.seq_lengths = seq_lengths\n",
        "    self.label_tensor = label_tensor\n",
        "    self.sampler = splr.BatchSampler(splr.RandomSampler(self.label_tensor), self.batch_size, False)\n",
        "    self.sampler_iter = iter(self.sampler)\n",
        "    \n",
        "  def __iter__(self):\n",
        "    self.sampler_iter = iter(self.sampler) # reset sampler iterator\n",
        "    return self\n",
        "\n",
        "  def _next_index(self):\n",
        "    return next(self.sampler_iter) # may raise StopIteration\n",
        "\n",
        "  def __next__(self):\n",
        "    index = self._next_index()\n",
        "\n",
        "    subset_seq_tensor = self.seq_tensor[index]\n",
        "    subset_seq_lengths = self.seq_lengths[index]\n",
        "    subset_label_tensor = self.label_tensor[index]\n",
        "\n",
        "    subset_seq_lengths, perm_idx = subset_seq_lengths.sort(0, descending=True)\n",
        "    subset_seq_tensor = subset_seq_tensor[perm_idx]\n",
        "    subset_label_tensor = subset_label_tensor[perm_idx]\n",
        "\n",
        "    return subset_seq_tensor, subset_seq_lengths, subset_label_tensor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sampler)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT6HTGm3HYu7",
        "colab_type": "code",
        "outputId": "5c18a85e-f01d-4fa2-d3b9-29589a0d6523",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "shuffled_idx = torch.randperm(label.shape[0])\n",
        "\n",
        "seq_tensor = seq_tensor[shuffled_idx]\n",
        "seq_lenghts = seq_lengths[shuffled_idx]\n",
        "label = label[shuffled_idx]\n",
        "\n",
        "PCT_TRAIN = 0.7\n",
        "PCT_VALID = 0.2\n",
        "\n",
        "length = len(label)\n",
        "train_seq_tensor = seq_tensor[:int(length*PCT_TRAIN)] \n",
        "train_seq_lengths = seq_lengths[:int(length*PCT_TRAIN)]\n",
        "train_label = label[:int(length*PCT_TRAIN)]\n",
        "\n",
        "valid_seq_tensor = seq_tensor[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
        "valid_seq_lengths = seq_lengths[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
        "valid_label = label[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))]\n",
        "\n",
        "test_seq_tensor = seq_tensor[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "test_seq_lengths = seq_lengths[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "test_label = label[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "\n",
        "print(train_seq_tensor.shape)\n",
        "print(valid_seq_tensor.shape)\n",
        "print(test_seq_tensor.shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([7000, 39470])\n",
            "torch.Size([2000, 39470])\n",
            "torch.Size([1000, 39470])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ol-GoapHYvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set shuffle = False since data is already shuffled\n",
        "batch_size = 50\n",
        "train_loader = CustomDataLoader(train_seq_tensor, train_seq_lengths, train_label, batch_size)\n",
        "valid_loader = CustomDataLoader(valid_seq_tensor, valid_seq_lengths, valid_label, batch_size)\n",
        "test_loader = CustomDataLoader(test_seq_tensor, test_seq_lengths, test_label, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43RbDf-OyKpg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "9d45d811-2cee-413f-f630-de354152982f"
      },
      "source": [
        "print(next(train_loader))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[  24,  243,  100,  ...,    0,    0,    0],\n",
            "        [  24,    6,    1,  ...,    0,    0,    0],\n",
            "        [  24, 1984,  568,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [  24,   88,  147,  ...,    0,    0,    0],\n",
            "        [  24,   54,   18,  ...,    0,    0,    0],\n",
            "        [  24, 8299,  200,  ...,    0,    0,    0]]), tensor([2353, 1262, 1088,  783,  463,  428,  417,  413,  401,  300,  282,  282,\n",
            "         278,  272,  270,  226,  193,  192,  174,  173,  173,  162,  152,  151,\n",
            "         149,  144,  143,  142,  119,  108,   88,   85,   84,   82,   81,   74,\n",
            "          69,   65,   62,   54,   50,   49,   44,   44,   40,   34,   34,   26,\n",
            "          14,    9]), tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
            "        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
            "        1, 0], dtype=torch.int16))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgU1rRGlHYvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Model\n",
        "'''\n",
        "1) Embedding Layer\n",
        "2) LSTM\n",
        "3) Fully Connected Layer\n",
        "4) Sigmoid Activation\n",
        "'''\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class SpamHamLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers,\\\n",
        "                 drop_lstm=0.2, drop_out = 0.3):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_lstm, batch_first=True)\n",
        "        \n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, seq_lengths):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "        #h, c  = self.init_hidden(x.size(0))\n",
        "        \n",
        "        # embeddings and lstm_out\n",
        "        embedded_seq_tensor = self.embedding(x)\n",
        "                \n",
        "        packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\n",
        "        \n",
        "        packed_output, (ht, ct) = self.lstm(packed_input, None)\n",
        "        \n",
        "        output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
        "        # output : batch_size X max_seq_len X hidden_dim\n",
        "        \n",
        "        \n",
        "        # stack up lstm outputs\n",
        "        output = output.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc(output)\n",
        "              \n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        output = output.view(batch_size, -1)\n",
        "        output = output[:, -1] # get last batch of labels\n",
        "        \n",
        "        # sigmoid function\n",
        "        output = self.sig(output)\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return output\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\\\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        return hidden\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0JOX-wrHYvT",
        "colab_type": "code",
        "outputId": "d867fbb9-4756-4cda-e78e-53dc5351dd6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "\n",
        "vocab_size = len(vocab_int)\n",
        "output_size = 1\n",
        "embedding_dim = 100 # int(vocab_size ** 0.25) # 15\n",
        "hidden_dim = 20\n",
        "n_layers = 5\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
        "net = SpamHamLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, \\\n",
        "                 0.2, 0.2)\n",
        "net = net.to(device)\n",
        "print(net)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SpamHamLSTM(\n",
            "  (embedding): Embedding(85782, 100)\n",
            "  (lstm): LSTM(100, 20, num_layers=5, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.2)\n",
            "  (fc): Linear(in_features=20, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc-EAnv1HYva",
        "colab_type": "code",
        "outputId": "e5979949-2c23-4d7b-d6fe-b990745eb284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.03\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "# training params\n",
        "\n",
        "epochs = 10 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "counter = 0\n",
        "print_every = 10\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    # h = net.init_hidden(batch_size)\n",
        "\n",
        "    net.zero_grad()\n",
        "    # batch loop\n",
        "    \n",
        "    for seq_tensor, seq_tensor_lengths, label in iter(train_loader):\n",
        "        counter += 1\n",
        "               \n",
        "        # https://github.com/pytorch/pytorch/issues/7236\n",
        "        # The error states that it expects a torch.cuda.LongTensor, \n",
        "        # while you gave it a torch.LongTensor (a CPU array). \n",
        "        # Move the inputs to the GPU using the .cuda() method and the error should go away.\n",
        "        seq_tensor = seq_tensor.to(device)\n",
        "        seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "        label = label.to(device)\n",
        " \n",
        "        # get the output from the model\n",
        "        output = net(seq_tensor, seq_tensor_lengths)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output, label.float())\n",
        "        optimizer.zero_grad() \n",
        "        loss.backward()\n",
        "        \n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            #val_h = net.init_hidden(batch_size)\n",
        "            \n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            \n",
        "            for seq_tensor, seq_tensor_lengths, label in iter(valid_loader):\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                # val_h = tuple([each.data for each in val_h])\n",
        "                \n",
        "                \n",
        "                seq_tensor = seq_tensor.to(device)\n",
        "                seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "                label = label.to(device)\n",
        "            \n",
        "                \n",
        "                output = net(seq_tensor, seq_tensor_lengths)\n",
        "                val_loss = criterion(output, label.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10... Step: 10... Loss: 0.694397... Val Loss: 0.693086\n",
            "Epoch: 1/10... Step: 20... Loss: 0.693789... Val Loss: 0.693048\n",
            "Epoch: 1/10... Step: 30... Loss: 0.695010... Val Loss: 0.693013\n",
            "Epoch: 1/10... Step: 40... Loss: 0.694909... Val Loss: 0.693150\n",
            "Epoch: 1/10... Step: 50... Loss: 0.700169... Val Loss: 0.693950\n",
            "Epoch: 1/10... Step: 60... Loss: 0.694288... Val Loss: 0.694061\n",
            "Epoch: 1/10... Step: 70... Loss: 0.693994... Val Loss: 0.693066\n",
            "Epoch: 1/10... Step: 80... Loss: 0.691942... Val Loss: 0.693144\n",
            "Epoch: 1/10... Step: 90... Loss: 0.686288... Val Loss: 0.693547\n",
            "Epoch: 1/10... Step: 100... Loss: 0.690337... Val Loss: 0.694826\n",
            "Epoch: 1/10... Step: 110... Loss: 0.695417... Val Loss: 0.694423\n",
            "Epoch: 1/10... Step: 120... Loss: 0.697981... Val Loss: 0.693249\n",
            "Epoch: 1/10... Step: 130... Loss: 0.698370... Val Loss: 0.694492\n",
            "Epoch: 1/10... Step: 140... Loss: 0.690111... Val Loss: 0.695615\n",
            "Epoch: 2/10... Step: 150... Loss: 0.704378... Val Loss: 0.694325\n",
            "Epoch: 2/10... Step: 160... Loss: 0.698408... Val Loss: 0.693748\n",
            "Epoch: 2/10... Step: 170... Loss: 0.687690... Val Loss: 0.694128\n",
            "Epoch: 2/10... Step: 180... Loss: 0.692389... Val Loss: 0.693196\n",
            "Epoch: 2/10... Step: 190... Loss: 0.693073... Val Loss: 0.693219\n",
            "Epoch: 2/10... Step: 200... Loss: 0.694586... Val Loss: 0.693234\n",
            "Epoch: 2/10... Step: 210... Loss: 0.693069... Val Loss: 0.693237\n",
            "Epoch: 2/10... Step: 220... Loss: 0.693219... Val Loss: 0.693156\n",
            "Epoch: 2/10... Step: 230... Loss: 0.692805... Val Loss: 0.693123\n",
            "Epoch: 2/10... Step: 240... Loss: 0.693424... Val Loss: 0.693149\n",
            "Epoch: 2/10... Step: 250... Loss: 0.690420... Val Loss: 0.693418\n",
            "Epoch: 2/10... Step: 260... Loss: 0.691064... Val Loss: 0.693297\n",
            "Epoch: 2/10... Step: 270... Loss: 0.692730... Val Loss: 0.693216\n",
            "Epoch: 2/10... Step: 280... Loss: 0.682357... Val Loss: 0.693620\n",
            "Epoch: 3/10... Step: 290... Loss: 0.698788... Val Loss: 0.693197\n",
            "Epoch: 3/10... Step: 300... Loss: 0.698103... Val Loss: 0.693920\n",
            "Epoch: 3/10... Step: 310... Loss: 0.693667... Val Loss: 0.693124\n",
            "Epoch: 3/10... Step: 320... Loss: 0.690544... Val Loss: 0.693185\n",
            "Epoch: 3/10... Step: 330... Loss: 0.693933... Val Loss: 0.693186\n",
            "Epoch: 3/10... Step: 340... Loss: 0.692014... Val Loss: 0.693364\n",
            "Epoch: 3/10... Step: 350... Loss: 0.691377... Val Loss: 0.693880\n",
            "Epoch: 3/10... Step: 360... Loss: 0.688301... Val Loss: 0.693157\n",
            "Epoch: 3/10... Step: 370... Loss: 0.691210... Val Loss: 0.692993\n",
            "Epoch: 3/10... Step: 380... Loss: 0.693651... Val Loss: 0.693053\n",
            "Epoch: 3/10... Step: 390... Loss: 0.695625... Val Loss: 0.693121\n",
            "Epoch: 3/10... Step: 400... Loss: 0.685732... Val Loss: 0.694246\n",
            "Epoch: 3/10... Step: 410... Loss: 0.703544... Val Loss: 0.694186\n",
            "Epoch: 3/10... Step: 420... Loss: 0.698102... Val Loss: 0.693594\n",
            "Epoch: 4/10... Step: 430... Loss: 0.694989... Val Loss: 0.693375\n",
            "Epoch: 4/10... Step: 440... Loss: 0.691468... Val Loss: 0.693533\n",
            "Epoch: 4/10... Step: 450... Loss: 0.690466... Val Loss: 0.693274\n",
            "Epoch: 4/10... Step: 460... Loss: 0.694559... Val Loss: 0.693148\n",
            "Epoch: 4/10... Step: 470... Loss: 0.693127... Val Loss: 0.693201\n",
            "Epoch: 4/10... Step: 480... Loss: 0.693878... Val Loss: 0.693146\n",
            "Epoch: 4/10... Step: 490... Loss: 0.698744... Val Loss: 0.693471\n",
            "Epoch: 4/10... Step: 500... Loss: 0.692295... Val Loss: 0.693426\n",
            "Epoch: 4/10... Step: 510... Loss: 0.692219... Val Loss: 0.694402\n",
            "Epoch: 4/10... Step: 520... Loss: 0.682889... Val Loss: 0.695387\n",
            "Epoch: 4/10... Step: 530... Loss: 0.694071... Val Loss: 0.693941\n",
            "Epoch: 4/10... Step: 540... Loss: 0.694323... Val Loss: 0.693454\n",
            "Epoch: 4/10... Step: 550... Loss: 0.699898... Val Loss: 0.693403\n",
            "Epoch: 4/10... Step: 560... Loss: 0.689997... Val Loss: 0.695266\n",
            "Epoch: 5/10... Step: 570... Loss: 0.713552... Val Loss: 0.693742\n",
            "Epoch: 5/10... Step: 580... Loss: 0.693275... Val Loss: 0.693179\n",
            "Epoch: 5/10... Step: 590... Loss: 0.692474... Val Loss: 0.693347\n",
            "Epoch: 5/10... Step: 600... Loss: 0.694235... Val Loss: 0.693300\n",
            "Epoch: 5/10... Step: 610... Loss: 0.699425... Val Loss: 0.694050\n",
            "Epoch: 5/10... Step: 620... Loss: 0.691537... Val Loss: 0.693169\n",
            "Epoch: 5/10... Step: 630... Loss: 0.699997... Val Loss: 0.693585\n",
            "Epoch: 5/10... Step: 640... Loss: 0.697790... Val Loss: 0.693904\n",
            "Epoch: 5/10... Step: 650... Loss: 0.694672... Val Loss: 0.694930\n",
            "Epoch: 5/10... Step: 660... Loss: 0.693571... Val Loss: 0.693165\n",
            "Epoch: 5/10... Step: 670... Loss: 0.692509... Val Loss: 0.693469\n",
            "Epoch: 5/10... Step: 680... Loss: 0.702787... Val Loss: 0.693583\n",
            "Epoch: 5/10... Step: 690... Loss: 0.693338... Val Loss: 0.693164\n",
            "Epoch: 5/10... Step: 700... Loss: 0.696659... Val Loss: 0.693508\n",
            "Epoch: 6/10... Step: 710... Loss: 0.691973... Val Loss: 0.693274\n",
            "Epoch: 6/10... Step: 720... Loss: 0.693347... Val Loss: 0.693197\n",
            "Epoch: 6/10... Step: 730... Loss: 0.694098... Val Loss: 0.693157\n",
            "Epoch: 6/10... Step: 740... Loss: 0.693959... Val Loss: 0.693287\n",
            "Epoch: 6/10... Step: 750... Loss: 0.689890... Val Loss: 0.693521\n",
            "Epoch: 6/10... Step: 760... Loss: 0.697556... Val Loss: 0.693451\n",
            "Epoch: 6/10... Step: 770... Loss: 0.692379... Val Loss: 0.693687\n",
            "Epoch: 6/10... Step: 780... Loss: 0.689230... Val Loss: 0.693581\n",
            "Epoch: 6/10... Step: 790... Loss: 0.693149... Val Loss: 0.693192\n",
            "Epoch: 6/10... Step: 800... Loss: 0.692650... Val Loss: 0.693420\n",
            "Epoch: 6/10... Step: 810... Loss: 0.691231... Val Loss: 0.693364\n",
            "Epoch: 6/10... Step: 820... Loss: 0.692782... Val Loss: 0.693477\n",
            "Epoch: 6/10... Step: 830... Loss: 0.686191... Val Loss: 0.693815\n",
            "Epoch: 6/10... Step: 840... Loss: 0.695046... Val Loss: 0.693359\n",
            "Epoch: 7/10... Step: 850... Loss: 0.692294... Val Loss: 0.693257\n",
            "Epoch: 7/10... Step: 860... Loss: 0.692055... Val Loss: 0.693302\n",
            "Epoch: 7/10... Step: 870... Loss: 0.701570... Val Loss: 0.693604\n",
            "Epoch: 7/10... Step: 880... Loss: 0.691699... Val Loss: 0.693309\n",
            "Epoch: 7/10... Step: 890... Loss: 0.691057... Val Loss: 0.693824\n",
            "Epoch: 7/10... Step: 900... Loss: 0.690724... Val Loss: 0.694203\n",
            "Epoch: 7/10... Step: 910... Loss: 0.692735... Val Loss: 0.693455\n",
            "Epoch: 7/10... Step: 920... Loss: 0.693195... Val Loss: 0.693209\n",
            "Epoch: 7/10... Step: 930... Loss: 0.703309... Val Loss: 0.693538\n",
            "Epoch: 7/10... Step: 940... Loss: 0.689805... Val Loss: 0.693530\n",
            "Epoch: 7/10... Step: 950... Loss: 0.693493... Val Loss: 0.693140\n",
            "Epoch: 7/10... Step: 960... Loss: 0.691450... Val Loss: 0.693239\n",
            "Epoch: 7/10... Step: 970... Loss: 0.693261... Val Loss: 0.693141\n",
            "Epoch: 7/10... Step: 980... Loss: 0.693415... Val Loss: 0.693143\n",
            "Epoch: 8/10... Step: 990... Loss: 0.694792... Val Loss: 0.693280\n",
            "Epoch: 8/10... Step: 1000... Loss: 0.699565... Val Loss: 0.693385\n",
            "Epoch: 8/10... Step: 1010... Loss: 0.697422... Val Loss: 0.693236\n",
            "Epoch: 8/10... Step: 1020... Loss: 0.693279... Val Loss: 0.693147\n",
            "Epoch: 8/10... Step: 1030... Loss: 0.693485... Val Loss: 0.693151\n",
            "Epoch: 8/10... Step: 1040... Loss: 0.692598... Val Loss: 0.693219\n",
            "Epoch: 8/10... Step: 1050... Loss: 0.693390... Val Loss: 0.693681\n",
            "Epoch: 8/10... Step: 1060... Loss: 0.702349... Val Loss: 0.694431\n",
            "Epoch: 8/10... Step: 1070... Loss: 0.692369... Val Loss: 0.693808\n",
            "Epoch: 8/10... Step: 1080... Loss: 0.692422... Val Loss: 0.693185\n",
            "Epoch: 8/10... Step: 1090... Loss: 0.693144... Val Loss: 0.693206\n",
            "Epoch: 8/10... Step: 1100... Loss: 0.692942... Val Loss: 0.693190\n",
            "Epoch: 8/10... Step: 1110... Loss: 0.694851... Val Loss: 0.693208\n",
            "Epoch: 8/10... Step: 1120... Loss: 0.691750... Val Loss: 0.693285\n",
            "Epoch: 9/10... Step: 1130... Loss: 0.692459... Val Loss: 0.693324\n",
            "Epoch: 9/10... Step: 1140... Loss: 0.694878... Val Loss: 0.693169\n",
            "Epoch: 9/10... Step: 1150... Loss: 0.705903... Val Loss: 0.694688\n",
            "Epoch: 9/10... Step: 1160... Loss: 0.692382... Val Loss: 0.694266\n",
            "Epoch: 9/10... Step: 1170... Loss: 0.697067... Val Loss: 0.693245\n",
            "Epoch: 9/10... Step: 1180... Loss: 0.693203... Val Loss: 0.693163\n",
            "Epoch: 9/10... Step: 1190... Loss: 0.693238... Val Loss: 0.693178\n",
            "Epoch: 9/10... Step: 1200... Loss: 0.693290... Val Loss: 0.693144\n",
            "Epoch: 9/10... Step: 1210... Loss: 0.694983... Val Loss: 0.693199\n",
            "Epoch: 9/10... Step: 1220... Loss: 0.693227... Val Loss: 0.693158\n",
            "Epoch: 9/10... Step: 1230... Loss: 0.694296... Val Loss: 0.693339\n",
            "Epoch: 9/10... Step: 1240... Loss: 0.691038... Val Loss: 0.693566\n",
            "Epoch: 9/10... Step: 1250... Loss: 0.697094... Val Loss: 0.693393\n",
            "Epoch: 9/10... Step: 1260... Loss: 0.693678... Val Loss: 0.693310\n",
            "Epoch: 10/10... Step: 1270... Loss: 0.692621... Val Loss: 0.695529\n",
            "Epoch: 10/10... Step: 1280... Loss: 0.693436... Val Loss: 0.697388\n",
            "Epoch: 10/10... Step: 1290... Loss: 0.681615... Val Loss: 0.695777\n",
            "Epoch: 10/10... Step: 1300... Loss: 0.693924... Val Loss: 0.693228\n",
            "Epoch: 10/10... Step: 1310... Loss: 0.697437... Val Loss: 0.693896\n",
            "Epoch: 10/10... Step: 1320... Loss: 0.696024... Val Loss: 0.694686\n",
            "Epoch: 10/10... Step: 1330... Loss: 0.710571... Val Loss: 0.697258\n",
            "Epoch: 10/10... Step: 1340... Loss: 0.693047... Val Loss: 0.694362\n",
            "Epoch: 10/10... Step: 1350... Loss: 0.696975... Val Loss: 0.693062\n",
            "Epoch: 10/10... Step: 1360... Loss: 0.705945... Val Loss: 0.695060\n",
            "Epoch: 10/10... Step: 1370... Loss: 0.696699... Val Loss: 0.694505\n",
            "Epoch: 10/10... Step: 1380... Loss: 0.696612... Val Loss: 0.693355\n",
            "Epoch: 10/10... Step: 1390... Loss: 0.692956... Val Loss: 0.693121\n",
            "Epoch: 10/10... Step: 1400... Loss: 0.696945... Val Loss: 0.693411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYPUIS0RHYvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers,\\\n",
        "                 drop_lstm=0.2, drop_out = 0.3, train_on_gpu = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sOv53x8HYvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html\n",
        "    That is, the embedding vector dimension should be the 4th root of the number of categories. Since our vocabulary size in this example is 81, the recommended number of dimensions is 3:\n",
        "        Note that this is just a general guideline; you can set the number of embedding dimensions as you please.\n",
        "        \n",
        "        \n",
        "        https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
        "            \n",
        "            So what about size of the hidden layer(s)--how many neurons? There are some empirically-derived rules-of-thumb, of these, the most commonly relied on is 'the optimal size of the hidden layer is usually between the size of the input and size of the output layers'. Jeff Heaton, author of Introduction to Neural Networks in Java offers a few more."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}