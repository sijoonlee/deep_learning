{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def pathToList(path = \"data/enron1/ham/*.txt\", unnecessary = [\"-\", \".\", \",\", \"/\", \":\", \"@\"]):\n",
    "    files  = glob.glob(path)\n",
    "    content_list = []\n",
    "    for file in files:\n",
    "        with open(file, encoding=\"ISO-8859-1\") as f:\n",
    "            content = f.read()\n",
    "            content = content.lower()\n",
    "            if len(unnecessary) is not 0:\n",
    "                content = ''.join([c for c in content if c not in unnecessary])\n",
    "            content_list.append(content)\n",
    "    \n",
    "    return content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8033\n"
     ]
    }
   ],
   "source": [
    "# Collect Ham data\n",
    "ham_paths = [\"data/enron1/ham/*.txt\", \"data/enron2/ham/*.txt\"]\n",
    "\n",
    "ham = pathToList(ham_paths[0])\n",
    "\n",
    "for index in range(1, len(ham_paths)):\n",
    "    ham = ham + pathToList(ham_paths[index])\n",
    "    \n",
    "print(len(ham))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2996\n"
     ]
    }
   ],
   "source": [
    "# Collect Spam data\n",
    "spam_paths = [\"data/enron1/spam/*.txt\", \"data/enron2/spam/*.txt\"]\n",
    "\n",
    "spam = pathToList(spam_paths[0])\n",
    "\n",
    "for index in range(1, len(spam_paths)):\n",
    "    \n",
    "    spam = spam + pathToList(path = spam_paths[index])\n",
    "    \n",
    "print(len(spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Issue occured in my computer\n",
    "# Decrease the number of data in Ham set\n",
    "import random\n",
    "random.shuffle(ham)\n",
    "ham = ham[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "arg: ham or spam data (numpy array)\n",
    "return: int dictionary [ word_n: count_n, ... ]\n",
    "'''\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab_int_dict(listed_data):\n",
    "    \n",
    "    # tokenize\n",
    "    all_words = []\n",
    "    for email in listed_data:\n",
    "        words = email.split()\n",
    "        all_words = all_words + words\n",
    "    \n",
    "    # Count\n",
    "    count_words = Counter(all_words)\n",
    "    \n",
    "    # Sort by Freq\n",
    "    sorted_words = count_words.most_common(len(count_words))\n",
    "    \n",
    "    vocab_int_dict = {word : index+1 for index, (word, count) in enumerate(sorted_words)}\n",
    "        # index starts from 1, since 0 is reserved for padding\n",
    "    \n",
    "    return vocab_int_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(listed_data, vocab_int_dict):\n",
    "    encoded_words = []\n",
    "    for email in listed_data:\n",
    "        item = [vocab_int_dict[word] for word in email.split()]\n",
    "        encoded_words.append(item)\n",
    "        \n",
    "    return encoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_encoded_words = encode_words(ham, build_vocab_int_dict(ham))\n",
    "spam_encoded_words = encode_words(spam, build_vocab_int_dict(spam))\n",
    "all_encoded_words = ham_encoded_words + spam_encoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5996\n"
     ]
    }
   ],
   "source": [
    "print(len(all_encoded_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_label = [0 for _ in range(len(ham))]\n",
    "spam_label = [1 for _ in range(len(spam))]\n",
    "all_label = ham_label + spam_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5996\n"
     ]
    }
   ],
   "source": [
    "print(len(all_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "arg: encoded_words : list of lists\n",
    "'''\n",
    "def padding(encoded_words):\n",
    "    sorted_encoded_words = sorted(encoded_words, key=lambda x:len(x))\n",
    "    size = len(sorted_encoded_words[-1]) # the longest one will be the size of input to the model\n",
    "    for i, x in enumerate(encoded_words):\n",
    "        missing = size - len(x)\n",
    "        encoded_words[i] = encoded_words[i] + [0 for _ in range(missing)] # 0 is padding\n",
    "        \n",
    "    return encoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = padding(all_encoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# python list\n",
    "# shuffle two lists at the same time\n",
    "def shuffle(a, b):\n",
    "    c = list(zip(a,b))\n",
    "    random.shuffle(c)\n",
    "    a, b = zip(*c)\n",
    "    return a, b\n",
    "\n",
    "# np array\n",
    "# assume that a.shape is eqaual to b.shape\n",
    "import numpy as np\n",
    "def np_shuffle(a, b):\n",
    "    indices = np.arange(a.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    return a[indices], b[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5996, 5420) (5996,)\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array(padded)\n",
    "labels = np.array(all_label)\n",
    "inputs, labels = np_shuffle(inputs, labels)\n",
    "print(inputs.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4197 1199 600\n",
      "5996\n"
     ]
    }
   ],
   "source": [
    "PCT_TRAIN = 0.7\n",
    "PCT_VALID = 0.2\n",
    "\n",
    "length = len(labels)\n",
    "train_x = inputs[:int(length*PCT_TRAIN)] \n",
    "train_y = labels[:int(length*PCT_TRAIN)]\n",
    "\n",
    "valid_x = inputs[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
    "valid_y = labels[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))]\n",
    "\n",
    "test_x = inputs[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
    "test_y = labels[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
    "\n",
    "print(len(train_y), len(valid_y), len(test_y))\n",
    "print(len(train_y)+len(valid_y)+len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"data/train_x.csv\", train_x, delimiter=\",\")\n",
    "np.savetxt(\"data/train_y.csv\", train_y, delimiter=\",\")\n",
    "np.savetxt(\"data/valid_x.csv\", valid_x, delimiter=\",\")\n",
    "np.savetxt(\"data/valid_y.csv\", valid_y, delimiter=\",\")\n",
    "np.savetxt(\"data/test_x.csv\", test_x, delimiter=\",\")\n",
    "np.savetxt(\"data/test_y.csv\", test_y, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# using as_tensor() method to avoid copy (save memory)\n",
    "train_data = TensorDataset(torch.as_tensor(train_x), torch.as_tensor(train_y))\n",
    "valid_data = TensorDataset(torch.as_tensor(valid_x), torch.as_tensor(valid_y))\n",
    "test_data = TensorDataset(torch.as_tensor(test_x), torch.as_tensor(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "argument\n",
    "    data: numpy array\n",
    "    shuffle: True or False\n",
    "    batch_size: batch size\n",
    "return\n",
    "    DataLoader object\n",
    "'''\n",
    "def prep_loader(data, shuffle, batch_size):\n",
    "    loader = DataLoader(data, shuffle = shuffle, batch_size = batch_size)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set shuffle = False since data is already shuffled\n",
    "batch_size = 30\n",
    "train_loader = prep_loader(train_data, False, 30)\n",
    "valid_loader = prep_loader(valid_data, False, 30)\n",
    "test_loader = prep_loader(test_data, False, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 5420])\n",
      "tensor([[   20,   457,   129,  ...,     0,     0,     0],\n",
      "        [   14,   622, 13133,  ...,     0,     0,     0]])\n",
      "torch.Size([30])\n",
      "tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "# make sure it iterates\n",
    "data_iter = iter(train_loader)\n",
    "x, y = data_iter.next()\n",
    "print(x.shape)\n",
    "print(x[:2])\n",
    "print(y.shape)\n",
    "print(y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "'''\n",
    "1) Embedding Layer\n",
    "2) LSTM\n",
    "3) Fully Connected Layer\n",
    "4) Sigmoid Activation \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
