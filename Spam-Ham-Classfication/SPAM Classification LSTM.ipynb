{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def pathToList(path = \"data/enron1/ham/*.txt\", unnecessary = [\"-\", \".\", \",\", \"/\", \":\", \"@\"]):\n",
    "    files  = glob.glob(path)\n",
    "    content_list = []\n",
    "    for file in files:\n",
    "        with open(file, encoding=\"ISO-8859-1\") as f:\n",
    "            content = f.read()\n",
    "            content = content.lower()\n",
    "            if len(unnecessary) is not 0:\n",
    "                content = ''.join([c for c in content if c not in unnecessary])\n",
    "            content_list.append(content)\n",
    "    \n",
    "    return content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8033\n"
     ]
    }
   ],
   "source": [
    "# Collect Ham data\n",
    "ham_paths = [\"data/enron1/ham/*.txt\", \"data/enron2/ham/*.txt\"]\n",
    "\n",
    "ham = pathToList(ham_paths[0])\n",
    "\n",
    "for index in range(1, len(ham_paths)):\n",
    "    ham = ham + pathToList(ham_paths[index])\n",
    "    \n",
    "print(len(ham))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2996\n"
     ]
    }
   ],
   "source": [
    "# Collect Spam data\n",
    "spam_paths = [\"data/enron1/spam/*.txt\", \"data/enron2/spam/*.txt\"]\n",
    "\n",
    "spam = pathToList(spam_paths[0])\n",
    "\n",
    "for index in range(1, len(spam_paths)):\n",
    "    \n",
    "    spam = spam + pathToList(path = spam_paths[index])\n",
    "    \n",
    "print(len(spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Issue occured in my computer\n",
    "# Decrease the number of data in Ham set\n",
    "import random\n",
    "random.shuffle(ham)\n",
    "ham = ham[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_and_spam = ham + spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "arg: ham or spam data (numpy array)\n",
    "return: int dictionary [ word_n: count_n, ... ]\n",
    "'''\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab_int_dict(listed_data):\n",
    "    \n",
    "    # tokenize\n",
    "    all_words = []\n",
    "    for email in listed_data:\n",
    "        words = email.split()\n",
    "        all_words = all_words + words\n",
    "    \n",
    "    # Count\n",
    "    count_words = Counter(all_words)\n",
    "    \n",
    "    # Sort by Freq\n",
    "    sorted_words = count_words.most_common(len(count_words))\n",
    "    \n",
    "    vocab_int_dict = {word : index+1 for index, (word, count) in enumerate(sorted_words)}\n",
    "        # index starts from 1, since 0 is reserved for padding\n",
    "    \n",
    "    return vocab_int_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(listed_data, vocab_int_dict):\n",
    "    encoded_words = []\n",
    "    print(\"length of vocab_int_dict\", len(vocab_int_dict))\n",
    "    for email in listed_data:\n",
    "        item = [vocab_int_dict[word] for word in email.split()]\n",
    "        encoded_words.append(item)\n",
    "        \n",
    "    return encoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vocab_int_dict 63964\n"
     ]
    }
   ],
   "source": [
    "all_encoded_words = encode_words(ham_and_spam, build_vocab_int_dict(ham_and_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5996\n"
     ]
    }
   ],
   "source": [
    "print(len(all_encoded_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_label = [0 for _ in range(len(ham))]\n",
    "spam_label = [1 for _ in range(len(spam))]\n",
    "all_label = ham_label + spam_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5996\n"
     ]
    }
   ],
   "source": [
    "print(len(all_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "arg: encoded_words : list of lists\n",
    "'''\n",
    "def padding(encoded_words):\n",
    "    sorted_encoded_words = sorted(encoded_words, key=lambda x:len(x))\n",
    "    size = len(sorted_encoded_words[-1]) # the longest one will be the size of input to the model\n",
    "    for i, x in enumerate(encoded_words):\n",
    "        missing = size - len(x)\n",
    "        encoded_words[i] = encoded_words[i] + [0 for _ in range(missing)] # 0 is padding\n",
    "        \n",
    "    return encoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = padding(all_encoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# python list\n",
    "# shuffle two lists at the same time\n",
    "def shuffle(a, b):\n",
    "    c = list(zip(a,b))\n",
    "    random.shuffle(c)\n",
    "    a, b = zip(*c)\n",
    "    return a, b\n",
    "\n",
    "# np array\n",
    "# assume that a.shape is eqaual to b.shape\n",
    "import numpy as np\n",
    "def np_shuffle(a, b):\n",
    "    indices = np.arange(a.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    return a[indices], b[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5996, 7563) (5996,)\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array(padded)\n",
    "labels = np.array(all_label)\n",
    "inputs, labels = np_shuffle(inputs, labels)\n",
    "print(inputs.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4197, 7563)\n",
      "4197 1199 600\n",
      "5996\n"
     ]
    }
   ],
   "source": [
    "PCT_TRAIN = 0.7\n",
    "PCT_VALID = 0.2\n",
    "\n",
    "length = len(labels)\n",
    "train_x = inputs[:int(length*PCT_TRAIN)] \n",
    "train_y = labels[:int(length*PCT_TRAIN)]\n",
    "\n",
    "valid_x = inputs[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
    "valid_y = labels[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))]\n",
    "\n",
    "test_x = inputs[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
    "test_y = labels[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
    "\n",
    "print(train_x.shape)\n",
    "print(len(train_y), len(valid_y), len(test_y))\n",
    "print(len(train_y)+len(valid_y)+len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# using as_tensor() method to avoid copy (save memory)\n",
    "train_data = TensorDataset(torch.as_tensor(train_x), torch.as_tensor(train_y))\n",
    "valid_data = TensorDataset(torch.as_tensor(valid_x), torch.as_tensor(valid_y))\n",
    "test_data = TensorDataset(torch.as_tensor(test_x), torch.as_tensor(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "argument\n",
    "    data: numpy array\n",
    "    shuffle: True or False\n",
    "    batch_size: batch size\n",
    "return\n",
    "    DataLoader object\n",
    "'''\n",
    "def prep_loader(data, shuffle, batch_size):\n",
    "    loader = DataLoader(data, shuffle = shuffle, batch_size = batch_size)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set shuffle = False since data is already shuffled\n",
    "batch_size = 30\n",
    "train_loader = prep_loader(train_data, False, 30)\n",
    "valid_loader = prep_loader(valid_data, False, 30)\n",
    "test_loader = prep_loader(test_data, False, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 7563])\n",
      "tensor([[  18,  223, 1771,  ...,    0,    0,    0],\n",
      "        [  18, 7220,  560,  ...,    0,    0,    0]])\n",
      "torch.Size([30])\n",
      "tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "# make sure it iterates\n",
    "data_iter = iter(train_loader)\n",
    "x, y = data_iter.next()\n",
    "print(x.shape)\n",
    "print(x[:2])\n",
    "print(y.shape)\n",
    "print(y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "'''\n",
    "1) Embedding Layer\n",
    "2) LSTM\n",
    "3) Fully Connected Layer\n",
    "4) Sigmoid Activation (0 or 1)\n",
    "'''\n",
    "import torch.nn as nn\n",
    "\n",
    "class SpamHamLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers,\\\n",
    "                 drop_lstm=0.2, drop_out = 0.3, train_on_gpu = False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.train_on_gpu = train_on_gpu\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_lstm, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (self.train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpamHamLSTM(\n",
      "  (embedding): Embedding(63965, 15)\n",
      "  (lstm): LSTM(15, 256, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "\n",
    "vocab_size = 63964 + 1\n",
    "output_size = 1\n",
    "embedding_dim = int(vocab_size ** 0.25) # 15\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "net = SpamHamLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "train_on_gpu = False\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        # h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                inputs = inputs.type(torch.LongTensor)\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers,\\\n",
    "                 drop_lstm=0.2, drop_out = 0.3, train_on_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html\n",
    "    That is, the embedding vector dimension should be the 4th root of the number of categories. Since our vocabulary size in this example is 81, the recommended number of dimensions is 3:\n",
    "        Note that this is just a general guideline; you can set the number of embedding dimensions as you please.\n",
    "        \n",
    "        \n",
    "        https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "            \n",
    "            So what about size of the hidden layer(s)--how many neurons? There are some empirically-derived rules-of-thumb, of these, the most commonly relied on is 'the optimal size of the hidden layer is usually between the size of the input and size of the output layers'. Jeff Heaton, author of Introduction to Neural Networks in Java offers a few more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
