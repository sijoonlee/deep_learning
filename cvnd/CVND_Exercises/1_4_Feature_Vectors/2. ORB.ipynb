{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Oriented FAST and Rotated BRIEF (ORB)\n",
    "\n",
    "One of the most challenging problems in computer vision is object detection. Object detection is the ability to recognize particular objects in images and being able to determine the location of those objects within the images. For example, if we perform car detection in the image below, we will not only be interested in saying how many cars are there in the image but also *where* those cars are in the image.\n",
    "<br>\n",
    "<figure>\n",
    "  <img src = \"./in_cell_images/cars_road.jpg\" width = 80% style = \"border: thin silver solid; padding: 10px\">\n",
    "      <figcaption style = \"text-align: center; font-style: italic\">Fig1. - Car Detection.</figcaption>\n",
    "</figure> \n",
    "<br>\n",
    "In order to perform this object-based image analysis, we will use ORB. ORB is a very fast algorithm that creates feature vectors from detected keypoints. You've learned that ORB has some great properties, such as being invariant to rotations, changes in illumination, and noise. \n",
    "\n",
    "In this notebook, we will see these properties in action and implement the ORB algorithm to detect a person’s face in an image using facial keypoints.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Images and Importing Resources\n",
    "\n",
    "In the code below we will use OpenCV to load an image of a woman’s face, which will use as our training image. Since, the `cv2.imread()` function loads images as BGR we will convert our image to RGB so we can display it with the correct colors. As usual we will convert our BGR image to Gray scale for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [20,10]\n",
    "\n",
    "# Load the training image\n",
    "image = cv2.imread('./images/face.jpeg')\n",
    "\n",
    "# Convert the training image to RGB\n",
    "training_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert the training image to gray Scale\n",
    "training_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Display the images\n",
    "plt.subplot(121)\n",
    "plt.title('Original Training Image')\n",
    "plt.imshow(training_image)\n",
    "plt.subplot(122)\n",
    "plt.title('Gray Scale Training Image')\n",
    "plt.imshow(training_gray, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locating Keypoints\n",
    "\n",
    "The first step in the ORB algorithm is to locate all the keypoints in the training image. After the keypoints have been located, ORB creates their corresponding binary feature vectors and groups them together in the ORB descriptor.\n",
    "\n",
    "We will use OpenCV’s `ORB` class to locate the keypoints and create their corresponding ORB descriptor. The parameters of the ORB algorithm are setup using the `ORB_create()` function. The parameters of the `ORB_create()` function and their default values are given below:\n",
    "\n",
    "\n",
    "`cv2.ORB_create(nfeatures = 500,\n",
    "               scaleFactor = 1.2,\n",
    "\t\t       nlevels = 8,\n",
    "\t\t       edgeThreshold = 31,\n",
    "\t\t       firstLevel = 0,\n",
    "\t\t       WTA_K = 2,\n",
    "\t\t       scoreType = HARRIS_SCORE,\n",
    "\t\t       patchSize = 31,\n",
    "\t\t       fastThreshold = 20)`\n",
    "    \n",
    "Parameters:\n",
    "\n",
    "* **nfeatures** - *int*  \n",
    "Determines the maximum number of features (keypoints) to locate.\n",
    "\n",
    "\n",
    "* **scaleFactor** - *float*  \n",
    "Pyramid decimation ratio, must be greater than 1. ORB uses an image pyramid to find features, therefore you must provide the scale factor between each layer in the pyramid and the number of levels the pyramid has. A `scaleFactor = 2` means the classical pyramid, where each next level has 4x less pixels than the previous. A big scale factor will diminish the number of features found.\n",
    "\n",
    "\n",
    "* **nlevels** - *int*  \n",
    "The number of pyramid levels. The smallest level will have a linear size equal to input_image_linear_size/pow(scaleFactor, nlevels).\n",
    "\n",
    "\n",
    "* **edgeThreshold** - - *int*  \n",
    "The size of the border where features are not detected. Since the keypoints have a specific pixel size, the edges of images must be excluded from the search. The size of the `edgeThreshold` should be equal to or greater than the patchSize parameter.\n",
    "\n",
    "\n",
    "* **firstLevel** - *int*  \n",
    "This parameter allows you to determine which level should be treated as the first level in the pyramid. It should be 0 in the current implementation. Usually, the pyramid level with a scale of unity is considered the first level.\n",
    "\n",
    "\n",
    "* **WTA_K** - *int*  \n",
    "The number of random pixels used to produce each element of the oriented BRIEF descriptor. The possible values are 2, 3, and 4, with 2 being the default value. For example, a value of 3 means three random pixels are chosen at a time to compare their brightness. The index of the brightest pixel is returned. Since there are 3 pixels, the returned index will be either 0, 1, or 2.\n",
    "\n",
    "\n",
    "* **scoreType** - *int*  \n",
    "This parameter can be set to either HARRIS_SCORE or FAST_SCORE. The default HARRIS_SCORE means that the Harris corner algorithm is used to rank features. The score is used to only retain the best features. The FAST_SCORE produces slightly less stable keypoints, but it is a little faster to compute.\n",
    "\n",
    "\n",
    "* **patchSize** - *int*  \n",
    "Size of the patch used by the oriented BRIEF descriptor. Of course, on smaller pyramid layers the perceived image area covered by a feature will be larger.\n",
    "\n",
    "\n",
    "As we can see, the `cv2. ORB_create()`function supports a wide range of parameters. The first two arguments (`nfeatures` and ` scaleFactor`) are probably the ones you are most likely to change. The other parameters can be safely left at their default values and you will get good results.\n",
    "\n",
    "In the code below, we will use the `ORB_create()`function to set the maximum number of keypoints we want to detect to 200, and to set the pyramid decimation ratio to 2.1. We will then use the ` .detectAndCompute (image)`method to locate the keypoints in the given training `image`and to compute their corresponding ORB descriptor.  Finally, we will use the ` cv2.drawKeypoints()`function to visualize the keypoints found by the ORB algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import copy to make copies of the training image\n",
    "import copy\n",
    "\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [14.0, 7.0]\n",
    "\n",
    "# Set the parameters of the ORB algorithm by specifying the maximum number of keypoints to locate and\n",
    "# the pyramid decimation ratio\n",
    "orb = cv2.ORB_create(200, 2.0)\n",
    "\n",
    "# Find the keypoints in the gray scale training image and compute their ORB descriptor.\n",
    "# The None parameter is needed to indicate that we are not using a mask.\n",
    "keypoints, descriptor = orb.detectAndCompute(training_gray, None)\n",
    "\n",
    "# Create copies of the training image to draw our keypoints on\n",
    "keyp_without_size = copy.copy(training_image)\n",
    "keyp_with_size = copy.copy(training_image)\n",
    "\n",
    "# Draw the keypoints without size or orientation on one copy of the training image \n",
    "cv2.drawKeypoints(training_image, keypoints, keyp_without_size, color = (0, 255, 0))\n",
    "\n",
    "# Draw the keypoints with size and orientation on the other copy of the training image\n",
    "cv2.drawKeypoints(training_image, keypoints, keyp_with_size, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Display the image with the keypoints without size or orientation\n",
    "plt.subplot(121)\n",
    "plt.title('Keypoints Without Size or Orientation')\n",
    "plt.imshow(keyp_without_size)\n",
    "\n",
    "# Display the image with the keypoints with size and orientation\n",
    "plt.subplot(122)\n",
    "plt.title('Keypoints With Size and Orientation')\n",
    "plt.imshow(keyp_with_size)\n",
    "plt.show()\n",
    "\n",
    "# Print the number of keypoints detected\n",
    "print(\"\\nNumber of keypoints Detected: \", len(keypoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the right image, every keypoint has a center, a size, and an angle. The center determines the location of each keypoint in the image; the size of each keypoint is determined by the patch size used by BRIEF to create its feature vector; and the angle tells us the orientation of the keypoint as determined by rBRIEF.\n",
    "\n",
    "Once the keypoints for the training image have been found and their corresponding ORB descriptor has been calculated, the same thing can be done for the query image. In order to see the properties of the ORB algorithm more clearly, in the next sections we will use the same image as our training and query image.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Matching\n",
    "\n",
    "Once we have the ORB descriptors for *both* the training and query images, the final step is to perform keypoint matching between the two images using their corresponding ORB descriptors. This *matching* is usually performed by a matching function. One of the most commonly used matching functions is called *Brute-Force*.\n",
    "\n",
    "In the code below we will use OpenCV’s `BFMatcher ` class to compare the keypoints in the training and query images.. The parameters of the Brute-Force matcher are setup using the `cv2.BFMatcher()`function. The parameters of the `cv2.BFMatcher()`function and their default values are given below:\n",
    "\n",
    "\n",
    "\n",
    "`cv2.BFMatcher(normType = cv2.NORM_L2,\n",
    "\t\t \t  crossCheck = false)`\n",
    "\n",
    "Parameters:\n",
    "\n",
    "* **normType**  \n",
    "Specifies the metric used to determine the quality of the match. By default, `normType = cv2.NORM_L2`, which measures the distance between two descriptors.  However, for binary descriptors like the ones created by ORB, the Hamming metric is more suitable. The Hamming metric determines the distance by counting the number of dissimilar bits between the binary descriptors. When the ORB descriptor is created using `WTA_K = 2`, two random pixels are chosen and compared in brightness. The index of the brightest pixel is returned as either 0 or 1.  Such output only occupies 1 bit, and therefore the ` cv2.NORM_HAMMING` metric should be used.  If, on the other hand, the ORB descriptor is created using `WTA_K = 3`, three random pixels are chosen and compared in brightness. The index of the brightest pixel is returned as either 0, 1, or 2.  Such output will occupy 2 bits, and therefore a special variant of the Hamming distance, known as the `cv2.NORM_HAMMING2` (the 2 stands for 2 bits), should be used instead. Then, for any metric chosen, when comparing the keypoints in the training and query images, the pair with the smaller metric (distance between them) is considered the best match.\n",
    "\n",
    "\n",
    "* **crossCheck** - *bool* \n",
    "A Boolean variable and can be set to either `True` or `False`. Cross-checking is very useful for eliminating false matches. Cross-checking works by performing the matching procedure two times. The first time the keypoints in the training image are compared to those in the query image; the second time, however, the keypoints in the query image are compared to those in the training image (*i.e.* the comparison is done backwards). When cross-checking is enabled a match is considered valid only if keypoint *A* in the training image is the best match of keypoint *B* in the query image and vice-versa (that is, if keypoint *B* in the query image is the best match of point *A* in the training image). \n",
    "\n",
    "Once the parameters of the *BFMatcher* have been set, we can use the `.match(descriptors_train, descriptors_query)` method to find the matching  keypoints between the training and query images using their ORB descriptors. Finally, we will use the ` cv2.drawMatches ()` function to visualize the matching keypoints found by the Brute-Force matcher. This function stacks the training and query images horizontally and draws lines from the keypoints in the training image to their corresponding best matching keypoints in the query image. Remember that in order to see the properties of the ORB algorithm more clearly, in the following examples we will use the same image as our training and query image.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [14.0, 7.0]\n",
    "\n",
    "# Load the training image\n",
    "image1 = cv2.imread('./images/face.jpeg')\n",
    "\n",
    "# Load the query image\n",
    "image2 = cv2.imread('./images/face.jpeg')\n",
    "\n",
    "# Convert the training image to RGB\n",
    "training_image = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert the query image to RGB\n",
    "query_image = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the training and query images\n",
    "plt.subplot(121)\n",
    "plt.title('Training Image')\n",
    "plt.imshow(training_image)\n",
    "plt.subplot(122)\n",
    "plt.title('Query Image')\n",
    "plt.imshow(query_image)\n",
    "plt.show()\n",
    "\n",
    "# Convert the training image to gray scale\n",
    "training_gray = cv2.cvtColor(training_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the query image to gray scale\n",
    "query_gray = cv2.cvtColor(query_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Set the parameters of the ORB algorithm by specifying the maximum number of keypoints to locate and\n",
    "# the pyramid decimation ratio\n",
    "orb = cv2.ORB_create(1000, 2.0)\n",
    "\n",
    "# Find the keypoints in the gray scale training and query images and compute their ORB descriptor.\n",
    "# The None parameter is needed to indicate that we are not using a mask in either case.\n",
    "keypoints_train, descriptors_train = orb.detectAndCompute(training_gray, None)\n",
    "keypoints_query, descriptors_query = orb.detectAndCompute(query_gray, None)\n",
    "\n",
    "# Create a Brute Force Matcher object. Set crossCheck to True so that the BFMatcher will only return consistent\n",
    "# pairs. Such technique usually produces best results with minimal number of outliers when there are enough matches.\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "\n",
    "# Perform the matching between the ORB descriptors of the training image and the query image\n",
    "matches = bf.match(descriptors_train, descriptors_query)\n",
    "\n",
    "# The matches with shorter distance are the ones we want. So, we sort the matches according to distance\n",
    "matches = sorted(matches, key = lambda x : x.distance)\n",
    "\n",
    "# Connect the keypoints in the training image with their best matching keypoints in the query image.\n",
    "# The best matches correspond to the first elements in the sorted matches list, since they are the ones\n",
    "# with the shorter distance. We draw the first 300 mathces and use flags = 2 to plot the matching keypoints\n",
    "# without size or orientation.\n",
    "result = cv2.drawMatches(training_gray, keypoints_train, query_gray, keypoints_query, matches[:300], query_gray, flags = 2)\n",
    "\n",
    "# Display the best matching points\n",
    "plt.title('Best Matching Points')\n",
    "plt.imshow(result)\n",
    "plt.show()\n",
    "\n",
    "# Print the number of keypoints detected in the training image\n",
    "print(\"Number of Keypoints Detected In The Training Image: \", len(keypoints_train))\n",
    "\n",
    "# Print the number of keypoints detected in the query image\n",
    "print(\"Number of Keypoints Detected In The Query Image: \", len(keypoints_query))\n",
    "\n",
    "# Print total number of matching points between the training and query images\n",
    "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, since both the training and query images are the exactly the same, we expect that the same number of keypoints are found in both images, and that all the keypoints match. We can clearly see that this indeed the case, ORB has found the same number of keypoints in both images and the Brute-Force matcher has been able to correctly match all the keypoints in training and query images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORB's Main Properties\n",
    "\n",
    "We will now explore each of the main properties of the ORB algorithm:\n",
    "\n",
    "* Scale Invariance\n",
    "* Rotational Invariance\n",
    "* Illumination Invariance\n",
    "* Noise Invariance\n",
    "\n",
    "Again, in order to see the properties of the ORB algorithm more clearly, in the following examples we will use the same image as our training and query image. \n",
    "\n",
    "\n",
    "## Scale Invariance\n",
    "\n",
    "The ORB algorithm is scale invariant. This means that it is able to detect objects in images regardless of their size. To see this, we will now use our Brute-Force matcher to match points between the training image and a query image that is a ¼ the size of the original training image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [14.0, 7.0]\n",
    "\n",
    "# Load the training image\n",
    "image1 = cv2.imread('./images/face.jpeg')\n",
    "\n",
    "# Load the query image\n",
    "image2 = cv2.imread('./images/faceQS.png')\n",
    "\n",
    "# Convert the training image to RGB\n",
    "training_image = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert the query image to RGB\n",
    "query_image = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the images\n",
    "plt.subplot(121)\n",
    "plt.title('Training Image')\n",
    "plt.imshow(training_image)\n",
    "plt.subplot(122)\n",
    "plt.title('Query Image')\n",
    "plt.imshow(query_image)\n",
    "plt.show()\n",
    "\n",
    "# Convert the training image to gray scale\n",
    "training_gray = cv2.cvtColor(training_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the query image to gray scale\n",
    "query_gray = cv2.cvtColor(query_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Set the parameters of the ORB algorithm by specifying the maximum number of keypoints to locate and\n",
    "# the pyramid decimation ratio\n",
    "orb = cv2.ORB_create(1000, 2.0)\n",
    "\n",
    "# Find the keypoints in the gray scale training and query images and compute their ORB descriptor.\n",
    "# The None parameter is needed to indicate that we are not using a mask in either case.\n",
    "keypoints_train, descriptors_train = orb.detectAndCompute(training_gray, None)\n",
    "keypoints_query, descriptors_query = orb.detectAndCompute(query_gray, None)\n",
    "\n",
    "# Create a Brute Force Matcher object. Set crossCheck to True so that the BFMatcher will only return consistent\n",
    "# pairs. Such technique usually produces best results with minimal number of outliers when there are enough matches.\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "\n",
    "# Perform the matching between the ORB descriptors of the training image and the query image\n",
    "matches = bf.match(descriptors_train, descriptors_query)\n",
    "\n",
    "# The matches with shorter distance are the ones we want. So, we sort the matches according to distance\n",
    "matches = sorted(matches, key = lambda x : x.distance)\n",
    "\n",
    "# Connect the keypoints in the training image with their best matching keypoints in the query image.\n",
    "# The best matches correspond to the first elements in the sorted matches list, since they are the ones\n",
    "# with the shorter distance. We draw the first 30 mathces and use flags = 2 to plot the matching keypoints\n",
    "# without size or orientation.\n",
    "result = cv2.drawMatches(training_gray, keypoints_train, query_gray, keypoints_query, matches[:30], query_gray, flags = 2)\n",
    "\n",
    "# Display the best matching points\n",
    "plt.title('Best Matching Points')\n",
    "plt.imshow(result)\n",
    "plt.show()\n",
    "\n",
    "# Print the shape of the training image\n",
    "print('\\nThe Training Image has shape:', training_gray.shape)\n",
    "\n",
    "#Print the shape of the query image\n",
    "print('The Query Image has shape:', query_gray.shape)\n",
    "\n",
    "# Print the number of keypoints detected in the training image\n",
    "print(\"\\nNumber of Keypoints Detected In The Training Image: \", len(keypoints_train))\n",
    "\n",
    "# Print the number of keypoints detected in the query image\n",
    "print(\"Number of Keypoints Detected In The Query Image: \", len(keypoints_query))\n",
    "\n",
    "# Print total number of matching points between the training and query images\n",
    "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, notice that the training image is 553 x 471 pixels, while the query image is 138 x 117 pixels,  ¼ the size of the original training image. Also notice that the number of keypoints detected in the query image is only 65, much smaller than the 831 keypoints found in the training image. Nevertheless, we can see that our Brute-Force matcher can match most of the keypoints in the query image with their corresponding keypoints in the training image.\n",
    "\n",
    "## Rotational Invariance\n",
    "\n",
    "The ORB algorithm is also rotationally invariant. This means that it is able to detect objects in images regardless of their orientation. To see this, we will now use our Brute-Force matcher to match points between the training image and a query image that has been rotated by 90 degrees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [14.0, 7.0]\n",
    "\n",
    "# Load the training image\n",
    "image1 = cv2.imread('./images/face.jpeg')\n",
    "\n",
    "# Load the query image\n",
    "image2 = cv2.imread('./images/faceR.jpeg')\n",
    "\n",
    "# Convert the training image to RGB\n",
    "training_image = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert the query image to RGB\n",
    "query_image = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the images\n",
    "plt.subplot(121)\n",
    "plt.title('Training Image')\n",
    "plt.imshow(training_image)\n",
    "plt.subplot(122)\n",
    "plt.title('Query Image')\n",
    "plt.imshow(query_image)\n",
    "plt.show()\n",
    "\n",
    "# Convert the training image to gray scale\n",
    "training_gray = cv2.cvtColor(training_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the query image to gray scale\n",
    "query_gray = cv2.cvtColor(query_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Set the parameters of the ORB algorithm by specifying the maximum number of keypoints to locate and\n",
    "# the pyramid decimation ratio\n",
    "orb = cv2.ORB_create(1000, 2.0)\n",
    "\n",
    "# Find the keypoints in the gray scale training and query images and compute their ORB descriptor.\n",
    "# The None parameter is needed to indicate that we are not using a mask in either case.\n",
    "keypoints_train, descriptors_train = orb.detectAndCompute(training_gray, None)\n",
    "keypoints_query, descriptors_query = orb.detectAndCompute(query_gray, None)\n",
    "\n",
    "# Create a Brute Force Matcher object. Set crossCheck to True so that the BFMatcher will only return consistent\n",
    "# pairs. Such technique usually produces best results with minimal number of outliers when there are enough matches.\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "\n",
    "# Perform the matching between the ORB descriptors of the training image and the query image\n",
    "matches = bf.match(descriptors_train, descriptors_query)\n",
    "\n",
    "# The matches with shorter distance are the ones we want. So, we sort the matches according to distance\n",
    "matches = sorted(matches, key = lambda x : x.distance)\n",
    "\n",
    "# Connect the keypoints in the training image with their best matching keypoints in the query image.\n",
    "# The best matches correspond to the first elements in the sorted matches list, since they are the ones\n",
    "# with the shorter distance. We draw the first 100 mathces and use flags = 2 to plot the matching keypoints\n",
    "# without size or orientation.\n",
    "result = cv2.drawMatches(training_gray, keypoints_train, query_gray, keypoints_query, matches[:100], query_gray, flags = 2)\n",
    "\n",
    "# Display the best matching points\n",
    "plt.title('Best Matching Points')\n",
    "plt.imshow(result)\n",
    "plt.show()\n",
    "\n",
    "# Print the number of keypoints detected in the training image\n",
    "print(\"\\nNumber of Keypoints Detected In The Training Image: \", len(keypoints_train))\n",
    "\n",
    "# Print the number of keypoints detected in the query image\n",
    "print(\"Number of Keypoints Detected In The Query Image: \", len(keypoints_query))\n",
    "\n",
    "# Print total number of matching points between the training and query images\n",
    "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we see that the number of keypoints detected in both images is very similar, and that even though the query image is rotated, our Brute-Force matcher can still match about 78% of the keypoints found. Also, notice that most of the matching keypoints are close to particular facial features, such as the eyes, nose, and mouth.   \n",
    "\n",
    "## Illumination Invariance\n",
    "\n",
    "The ORB algorithm is also illumination invariant. This means that it is able to detect objects in images regardless of their illumination. To see this, we will now use our Brute-Force matcher to match points between the training image and a query image that is much brighter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [14.0, 7.0]\n",
    "\n",
    "# Load the training image\n",
    "image1 = cv2.imread('./images/face.jpeg')\n",
    "\n",
    "# Load the query image\n",
    "image2 = cv2.imread('./images/faceRI.png')\n",
    "\n",
    "# Convert the training image to RGB\n",
    "training_image = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert the query image to RGB\n",
    "query_image = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the images\n",
    "plt.subplot(121)\n",
    "plt.title('Training Image')\n",
    "plt.imshow(training_image)\n",
    "plt.subplot(122)\n",
    "plt.title('Query Image')\n",
    "plt.imshow(query_image)\n",
    "plt.show()\n",
    "\n",
    "# Convert the training image to gray scale\n",
    "training_gray = cv2.cvtColor(training_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the query image to gray scale\n",
    "query_gray = cv2.cvtColor(query_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Set the parameters of the ORB algorithm by specifying the maximum number of keypoints to locate and\n",
    "# the pyramid decimation ratio\n",
    "orb = cv2.ORB_create(1000, 2.0)\n",
    "\n",
    "# Find the keypoints in the gray scale training and query images and compute their ORB descriptor.\n",
    "# The None parameter is needed to indicate that we are not using a mask in either case.\n",
    "keypoints_train, descriptors_train = orb.detectAndCompute(training_gray, None)\n",
    "keypoints_query, descriptors_query = orb.detectAndCompute(query_gray, None)\n",
    "\n",
    "# Create a Brute Force Matcher object. Set crossCheck to True so that the BFMatcher will only return consistent\n",
    "# pairs. Such technique usually produces best results with minimal number of outliers when there are enough matches.\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "\n",
    "# Perform the matching between the ORB descriptors of the training image and the query image\n",
    "matches = bf.match(descriptors_train, descriptors_query)\n",
    "\n",
    "# The matches with shorter distance are the ones we want. So, we sort the matches according to distance\n",
    "matches = sorted(matches, key = lambda x : x.distance)\n",
    "\n",
    "# Connect the keypoints in the training image with their best matching keypoints in the query image.\n",
    "# The best matches correspond to the first elements in the sorted matches list, since they are the ones\n",
    "# with the shorter distance. We draw the first 100 mathces and use flags = 2 to plot the matching keypoints\n",
    "# without size or orientation.\n",
    "result = cv2.drawMatches(training_gray, keypoints_train, query_gray, keypoints_query, matches[:100], query_gray, flags = 2)\n",
    "\n",
    "# Display the best matching points\n",
    "plt.title('Best Matching Points')\n",
    "plt.imshow(result)\n",
    "plt.show()\n",
    "\n",
    "# Print the number of keypoints detected in the training image\n",
    "print(\"\\nNumber of Keypoints Detected In The Training Image: \", len(keypoints_train))\n",
    "\n",
    "# Print the number of keypoints detected in the query image\n",
    "print(\"Number of Keypoints Detected In The Query Image: \", len(keypoints_query))\n",
    "\n",
    "# Print total number of matching points between the training and query images\n",
    "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we see that the number of keypoints detected in both images is again very similar, and that even though the query image is much brighter, our Brute-Force matcher can still match about 63% of the keypoints found. \n",
    "\n",
    "## Noise Invariance\n",
    "\n",
    "The ORB algorithm is also noise invariant. This means that it is able to detect objects in images, even if the images have some degree of noise. To see this, we will now use our Brute-Force matcher to match points between the training image and a query image that has a lot of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [14.0, 7.0]\n",
    "\n",
    "# Load the training image\n",
    "image1 = cv2.imread('./images/face.jpeg')\n",
    "\n",
    "# Load the noisy, gray scale query image. \n",
    "image2 = cv2.imread('./images/faceRN5.png')\n",
    "\n",
    "# Convert the query image to gray scale\n",
    "query_gray = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the training image to gray scale\n",
    "training_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Display the images\n",
    "plt.subplot(121)\n",
    "plt.imshow(training_gray, cmap = 'gray')\n",
    "plt.title('Gray Scale Training Image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(query_gray, cmap = 'gray')\n",
    "plt.title('Query Image')\n",
    "plt.show()\n",
    "\n",
    "# Set the parameters of the ORB algorithm by specifying the maximum number of keypoints to locate and\n",
    "# the pyramid decimation ratio\n",
    "orb = cv2.ORB_create(1000, 1.3)\n",
    "\n",
    "# Find the keypoints in the gray scale training and query images and compute their ORB descriptor.\n",
    "# The None parameter is needed to indicate that we are not using a mask in either case. \n",
    "keypoints_train, descriptors_train = orb.detectAndCompute(training_gray, None)\n",
    "keypoints_query, descriptors_query = orb.detectAndCompute(query_gray, None)\n",
    "\n",
    "# Create a Brute Force Matcher object. We set crossCheck to True so that the BFMatcher will only return consistent\n",
    "# pairs. Such technique usually produces best results with minimal number of outliers when there are enough matches.\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "\n",
    "# Perform the matching between the ORB descriptors of the training image and the query image\n",
    "matches = bf.match(descriptors_train, descriptors_query)\n",
    "\n",
    "# The matches with shorter distance are the ones we want. So, we sort the matches according to distance\n",
    "matches = sorted(matches, key = lambda x : x.distance)\n",
    "\n",
    "# Connect the keypoints in the training image with their best matching keypoints in the query image.\n",
    "# The best matches correspond to the first elements in the sorted matches list, since they are the ones\n",
    "# with the shorter distance. We draw the first 100 mathces and use flags = 2 to plot the matching keypoints\n",
    "# without size or orientation.\n",
    "result = cv2.drawMatches(training_gray, keypoints_train, query_gray, keypoints_query, matches[:100], query_gray, flags = 2)\n",
    "\n",
    "# we display the image\n",
    "plt.title('Best Matching Points')\n",
    "plt.imshow(result)\n",
    "plt.show()\n",
    "\n",
    "# Print the number of keypoints detected in the training image\n",
    "print(\"Number of Keypoints Detected In The Training Image: \", len(keypoints_train))\n",
    "\n",
    "# Print the number of keypoints detected in the query image\n",
    "print(\"Number of Keypoints Detected In The Query Image: \", len(keypoints_query))\n",
    "\n",
    "# Print total number of matching points between the training and query images\n",
    "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, again we see that the number of keypoints detected in both images is very similar, and that even though the query image is has a lot of noise, our Brute-Force matcher can still match about 63% of the keypoints found.  Also, notice that most of the matching keypoints are close to particular facial features, such as the eyes, nose, and mouth. In addition, we can see that there are a few features that don’t quite match up, but may have been chosen because of similar patterns of intensity in that area of the image.  We will also like to point out that in this case we used a pyramid decimation ratio of 1.3, instead of the of value of 2.0 we used in the previous examples, because in this particular case, it produces better results. \n",
    "\n",
    "\n",
    "# Object Detection\n",
    "\n",
    "We will now implement the ORB algorithm to detect the face in the training image in another image. As usual, we will start by loading our training and query images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [14.0, 7.0]\n",
    "\n",
    "# Load the training image\n",
    "image1 = cv2.imread('./images/face.jpeg')\n",
    "\n",
    "# Load the query image\n",
    "image2 = cv2.imread('./images/Team.jpeg')\n",
    "\n",
    "# Convert the training image to RGB\n",
    "training_image = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert the query image to RGB\n",
    "query_image = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the images\n",
    "plt.subplot(121)\n",
    "plt.imshow(training_image)\n",
    "plt.title('Training Image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(query_image)\n",
    "plt.title('Query Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, the training image contains a face, therefore, as we have seen, the majority of the keypoints detected are close to facial features, such as the eyes, nose, and mouth. On the other hand, our query image is a picture of a group of people, one of which, is the woman we want to detect.  Let’s now detect the keypoints for the query image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [34.0, 34.0]\n",
    "\n",
    "# Convert the training image to gray scale\n",
    "training_gray = cv2.cvtColor(training_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the query image to gray scale\n",
    "query_gray = cv2.cvtColor(query_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Set the parameters of the ORB algorithm by specifying the maximum number of keypoints to locate and\n",
    "# the pyramid decimation ratio\n",
    "orb = cv2.ORB_create(5000, 2.0)\n",
    "\n",
    "# Find the keypoints in the gray scale training and query images and compute their ORB descriptor.\n",
    "# The None parameter is needed to indicate that we are not using a mask in either case.  \n",
    "keypoints_train, descriptors_train = orb.detectAndCompute(training_gray, None)\n",
    "keypoints_query, descriptors_query = orb.detectAndCompute(query_gray, None)\n",
    "\n",
    "# Create copies of the query images to draw our keypoints on\n",
    "query_img_keyp = copy.copy(query_image)\n",
    "\n",
    "# Draw the keypoints with size and orientation on the copy of the query image\n",
    "cv2.drawKeypoints(query_image, keypoints_query, query_img_keyp, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Display the query image with the keypoints with size and orientation\n",
    "plt.title('Keypoints With Size and Orientation', fontsize = 30)\n",
    "plt.imshow(query_img_keyp)\n",
    "plt.show()\n",
    "\n",
    "# Print the number of keypoints detected\n",
    "print(\"\\nNumber of keypoints Detected: \", len(keypoints_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the query image has keypoints in many parts of the image. Now that we have the keypoints and ORB descriptors of both the training  and query images, we can use a Brute-Force matcher to try to locate the woman’s face in the query image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [34.0, 34.0]\n",
    "\n",
    "# Create a Brute Force Matcher object. We set crossCheck to True so that the BFMatcher will only return consistent\n",
    "# pairs. Such technique usually produces best results with minimal number of outliers when there are enough matches.\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "\n",
    "# Perform the matching between the ORB descriptors of the training image and the query image\n",
    "matches = bf.match(descriptors_train, descriptors_query)\n",
    "\n",
    "# The matches with shorter distance are the ones we want. So, we sort the matches according to distance\n",
    "matches = sorted(matches, key = lambda x : x.distance)\n",
    "\n",
    "# Connect the keypoints in the training image with their best matching keypoints in the query image.\n",
    "# The best matches correspond to the first elements in the sorted matches list, since they are the ones\n",
    "# with the shorter distance. We draw the first 85 mathces and use flags = 2 to plot the matching keypoints\n",
    "# without size or orientation.\n",
    "result = cv2.drawMatches(training_gray, keypoints_train, query_gray, keypoints_query, matches[:85], query_gray, flags = 2)\n",
    "\n",
    "# we display the image\n",
    "plt.title('Best Matching Points', fontsize = 30)\n",
    "plt.imshow(result)\n",
    "plt.show()\n",
    "\n",
    "# Print the number of keypoints detected in the training image\n",
    "print(\"Number of Keypoints Detected In The Training Image: \", len(keypoints_train))\n",
    "\n",
    "# Print the number of keypoints detected in the query image\n",
    "print(\"Number of Keypoints Detected In The Query Image: \", len(keypoints_query))\n",
    "\n",
    "# Print total number of matching Keypoints between the training and query images\n",
    "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that even though there are many faces and objects in the query image, our Brute-Force matcher has been able to correctly locate the woman’s face in the query image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
